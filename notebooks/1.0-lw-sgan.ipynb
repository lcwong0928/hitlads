{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import asarray\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Activation\n",
    "from matplotlib import pyplot\n",
    "from keras import backend\n",
    "\n",
    "from keras.layers import *\n",
    "from keras import *\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/semi-supervised-generative-adversarial-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2718, 100, 25), (2718,))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, index = np.load('processed/data/X.npy'), np.load('processed/data/index.npy')\n",
    "X_train.shape, index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "for i in range(len(index)):\n",
    "    if index[i] < 1279823200 and (index[i] + 100 * 21600) > 1278168000:\n",
    "        y_train.append(1)\n",
    "    else:\n",
    "        y_train.append(0)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Discriminator Models With Shared Weights\n",
    "\n",
    "1-D convolutional layer for both Critics, with the intention of capturing local temporal features that can determine how anomalous a sequence is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the best way combine the surpvised + unsurpervised ?\n",
    "\n",
    "def define_discriminator():\n",
    "    in_shape = (100, 25)\n",
    "    in_image = Input(shape=in_shape)\n",
    "    fe = Conv1D(1, 5, activation='relu')(in_image)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    fe = Flatten()(fe)\n",
    "    fe = Dropout(0.4)(fe)\n",
    "\n",
    "    # unsupervised discriminator model\n",
    "    d_out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "    d_model = Model(in_image, d_out_layer)\n",
    "    d_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
    "\n",
    "    # supervised discriminator model\n",
    "    c_out_layer = Dense(2, activation='softmax')(fe)\n",
    "    c_model = Model(in_image, c_out_layer)\n",
    "    c_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "    return d_model, c_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, 100, 25)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 96, 1)             126       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)   (None, 96, 1)             0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 223\n",
      "Trainable params: 223\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, 100, 25)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 96, 1)             126       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)   (None, 96, 1)             0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 194       \n",
      "=================================================================\n",
      "Total params: 320\n",
      "Trainable params: 320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d_model, c_model = define_discriminator()\n",
    "d_model.summary()\n",
    "c_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_106\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_72 (InputLayer)        (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "sequential_12 (Sequential)   (None, 100, 25)           145933    \n",
      "=================================================================\n",
      "Total params: 145,933\n",
      "Trainable params: 145,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def define_generator():\n",
    "    inshape = (100, 20)\n",
    "    in_image = Input(shape=inshape)\n",
    "    # todo: change to bidirectional\n",
    "    fe = LSTM(25, activation='relu', return_sequences=True)(in_image)\n",
    "    model = Model(in_image, fe)\n",
    "    return model\n",
    "\n",
    "\n",
    "# input_shape = (100, 1)\n",
    "\n",
    "# def build_generator(input_shape, generator_reshape_dim=100, generator_reshape_shape=input_shape):\n",
    "#     x = Input(shape=input_shape)\n",
    "#     model = keras.models.Sequential()\n",
    "#     model.add(keras.layers.Flatten())\n",
    "#     model.add(keras.layers.Dense(units=generator_reshape_dim))\n",
    "#     model.add(keras.layers.Reshape(target_shape=generator_reshape_shape))\n",
    "#     model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode='concat'))\n",
    "#     model.add(keras.layers.convolutional.UpSampling1D(size=1))\n",
    "#     model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode='concat'))\n",
    "#     model.add(keras.layers.TimeDistributed(keras.layers.Dense(units=25)))\n",
    "#     model.add(keras.layers.Activation(activation='tanh'))\n",
    "#     return Model(x, model(x))\n",
    "              \n",
    "# generator = build_generator(input_shape)\n",
    "# generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model):\n",
    "    \n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    \n",
    "    # connect image output from generator as input to discriminator\n",
    "    gan_output = d_model(g_model.output)\n",
    "    \n",
    "    # define gan model as taking noise and outputting a classification\n",
    "    model = Model(g_model.input, gan_output)\n",
    "    \n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a supervised subset of the dataset, ensures classes are balanced\n",
    "def select_supervised_samples(dataset, n_samples=100, n_classes=2):\n",
    "    X, y = dataset\n",
    "    X_list, y_list = list(), list()\n",
    "    n_per_class = int(n_samples / n_classes)\n",
    "    for i in range(n_classes):\n",
    "        # get all images for this class\n",
    "        X_with_class = X[y == i]\n",
    "        # choose random instances\n",
    "        ix = randint(0, len(X_with_class), n_per_class)\n",
    "        # add to list\n",
    "        [X_list.append(X_with_class[j]) for j in ix]\n",
    "        [y_list.append(i) for j in ix]\n",
    "    return asarray(X_list), asarray(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # split into images and labels\n",
    "    images, labels = dataset\n",
    "    # choose random instances\n",
    "    ix = randint(0, images.shape[0], n_samples)\n",
    "    # select images and labels\n",
    "    X, labels = images[ix], labels[ix]\n",
    "    # generate class labels\n",
    "    y = ones((n_samples, 1))\n",
    "    return [X, labels], y\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(n_samples):\n",
    "    return np.array([np.random.normal(size=[100, 20]) for _ in range(n_samples)])\n",
    " \n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, n_samples):\n",
    "    # generate points in latent space\n",
    "    z_input = generate_latent_points(n_samples)\n",
    "    # predict outputs\n",
    "    images = generator.predict(z_input)\n",
    "    # create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return images, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, c_model, latent_dim, dataset, n_samples=100):\n",
    "    # prepare fake examples\n",
    "    X, _ = generate_fake_samples(g_model, n_samples)\n",
    "    # scale from [-1,1] to [0,1]\n",
    "    X = (X + 1) / 2.0\n",
    "    # plot images\n",
    "    for i in range(100):\n",
    "        # define subplot\n",
    "        pyplot.subplot(10, 10, 1 + i)\n",
    "        # turn off axis\n",
    "        pyplot.axis('off')\n",
    "        # plot raw pixel data\n",
    "        # pyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n",
    "    # save plot to file\n",
    "    filename1 = 'generated_plot_%04d.png' % (step+1)\n",
    "    pyplot.savefig(filename1)\n",
    "    pyplot.close()\n",
    "    # evaluate the classifier model\n",
    "    X, y = dataset\n",
    "    _, acc = c_model.evaluate(X, y, verbose=0)\n",
    "    print('Classifier Accuracy: %.3f%%' % (acc * 100))\n",
    "    # save the generator model\n",
    "    filename2 = 'g_model_%04d.h5' % (step+1)\n",
    "    g_model.save(filename2)\n",
    "    # save the classifier model\n",
    "    filename3 = 'c_model_%04d.h5' % (step+1)\n",
    "    c_model.save(filename3)\n",
    "    print('>Saved: %s, %s, and %s' % (filename1, filename2, filename3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, c_model, gan_model, dataset, latent_dim, n_epochs=20, n_batch=100):\n",
    "    # select supervised dataset\n",
    "    X_sup, y_sup = select_supervised_samples(dataset)\n",
    "    print(X_sup.shape, y_sup.shape)\n",
    "    \n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "    \n",
    "    # calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    \n",
    "    # calculate the size of half a batch of samples\n",
    "    half_batch = int(n_batch / 2)\n",
    "    print('n_epochs=%d, n_batch=%d, 1/2=%d, b/e=%d, steps=%d' % (n_epochs, n_batch, half_batch, bat_per_epo, n_steps))\n",
    "    \n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "        \n",
    "        # update supervised discriminator (c)\n",
    "        [Xsup_real, ysup_real], _ = generate_real_samples([X_sup, y_sup], half_batch)\n",
    "        c_loss, c_acc = c_model.train_on_batch(Xsup_real, ysup_real)\n",
    "        \n",
    "        # update unsupervised discriminator (d)\n",
    "        [X_real, _], y_real = generate_real_samples(dataset, half_batch)\n",
    "        d_loss1 = d_model.train_on_batch(X_real, y_real)\n",
    "        X_fake, y_fake = generate_fake_samples(g_model, half_batch)\n",
    "        d_loss2 = d_model.train_on_batch(X_fake, y_fake)\n",
    "        \n",
    "        # update generator (g)\n",
    "        X_gan, y_gan = generate_latent_points(n_batch), ones((n_batch, 1))\n",
    "        g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "        \n",
    "        # summarize loss on this batch\n",
    "        print('>%d, c[%.3f,%.0f], d[%.3f,%.3f], g[%.3f]' % (i+1, c_loss, c_acc*100, d_loss1, d_loss2, g_loss))\n",
    "        \n",
    "        # # evaluate the model performance every so often\n",
    "        # if (i+1) % (bat_per_epo * 1) == 0:\n",
    "        #     summarize_performance(i, g_model, c_model, latent_dim, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100, 25) (100,)\n",
      "n_epochs=20, n_batch=100, 1/2=50, b/e=27, steps=540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lcwong/opt/anaconda3/envs/orion-env/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/Users/lcwong/opt/anaconda3/envs/orion-env/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, c[0.734,36], d[0.722,0.623], g[0.808]\n",
      ">2, c[0.697,48], d[0.701,0.657], g[0.764]\n",
      ">3, c[0.682,40], d[0.703,0.616], g[0.782]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lcwong/opt/anaconda3/envs/orion-env/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">4, c[0.697,52], d[0.700,0.641], g[0.792]\n",
      ">5, c[0.714,54], d[0.703,0.637], g[0.804]\n",
      ">6, c[0.696,56], d[0.700,0.596], g[0.781]\n",
      ">7, c[0.713,52], d[0.698,0.591], g[0.805]\n",
      ">8, c[0.694,52], d[0.702,0.633], g[0.811]\n",
      ">9, c[0.692,56], d[0.701,0.589], g[0.798]\n",
      ">10, c[0.692,50], d[0.711,0.605], g[0.839]\n",
      ">11, c[0.705,50], d[0.704,0.612], g[0.794]\n",
      ">12, c[0.699,56], d[0.691,0.609], g[0.815]\n",
      ">13, c[0.716,42], d[0.691,0.623], g[0.809]\n",
      ">14, c[0.701,42], d[0.711,0.625], g[0.845]\n",
      ">15, c[0.694,56], d[0.698,0.641], g[0.805]\n",
      ">16, c[0.697,48], d[0.692,0.584], g[0.832]\n",
      ">17, c[0.698,58], d[0.694,0.597], g[0.849]\n",
      ">18, c[0.687,44], d[0.694,0.577], g[0.848]\n",
      ">19, c[0.690,42], d[0.691,0.569], g[0.849]\n",
      ">20, c[0.696,48], d[0.689,0.588], g[0.848]\n",
      ">21, c[0.678,68], d[0.695,0.592], g[0.833]\n",
      ">22, c[0.690,48], d[0.691,0.588], g[0.816]\n",
      ">23, c[0.684,58], d[0.695,0.598], g[0.846]\n",
      ">24, c[0.686,54], d[0.692,0.627], g[0.846]\n",
      ">25, c[0.691,44], d[0.689,0.595], g[0.797]\n",
      ">26, c[0.699,40], d[0.696,0.612], g[0.860]\n",
      ">27, c[0.698,48], d[0.701,0.564], g[0.857]\n",
      ">28, c[0.684,44], d[0.693,0.622], g[0.855]\n",
      ">29, c[0.691,52], d[0.698,0.593], g[0.842]\n",
      ">30, c[0.690,40], d[0.690,0.595], g[0.870]\n",
      ">31, c[0.696,44], d[0.693,0.566], g[0.845]\n",
      ">32, c[0.684,44], d[0.697,0.591], g[0.867]\n",
      ">33, c[0.697,60], d[0.690,0.589], g[0.893]\n",
      ">34, c[0.688,66], d[0.691,0.596], g[0.845]\n",
      ">35, c[0.689,54], d[0.693,0.577], g[0.834]\n",
      ">36, c[0.701,32], d[0.693,0.558], g[0.831]\n",
      ">37, c[0.692,34], d[0.694,0.595], g[0.842]\n",
      ">38, c[0.695,34], d[0.694,0.574], g[0.860]\n",
      ">39, c[0.692,56], d[0.695,0.574], g[0.849]\n",
      ">40, c[0.695,36], d[0.692,0.578], g[0.869]\n",
      ">41, c[0.691,44], d[0.693,0.551], g[0.854]\n",
      ">42, c[0.693,50], d[0.695,0.556], g[0.842]\n",
      ">43, c[0.692,42], d[0.693,0.582], g[0.886]\n",
      ">44, c[0.692,56], d[0.692,0.554], g[0.892]\n",
      ">45, c[0.692,46], d[0.693,0.606], g[0.877]\n",
      ">46, c[0.694,42], d[0.693,0.556], g[0.899]\n",
      ">47, c[0.693,50], d[0.693,0.530], g[0.871]\n",
      ">48, c[0.693,44], d[0.692,0.529], g[0.868]\n",
      ">49, c[0.693,42], d[0.692,0.555], g[0.852]\n",
      ">50, c[0.693,56], d[0.692,0.582], g[0.906]\n",
      ">51, c[0.693,54], d[0.692,0.546], g[0.862]\n",
      ">52, c[0.693,42], d[0.692,0.580], g[0.914]\n",
      ">53, c[0.693,48], d[0.692,0.542], g[0.918]\n",
      ">54, c[0.693,62], d[0.692,0.564], g[0.880]\n",
      ">55, c[0.693,38], d[0.692,0.561], g[0.875]\n",
      ">56, c[0.693,56], d[0.692,0.532], g[0.910]\n",
      ">57, c[0.693,48], d[0.692,0.555], g[0.932]\n",
      ">58, c[0.693,46], d[0.692,0.557], g[0.921]\n",
      ">59, c[0.693,60], d[0.692,0.567], g[0.901]\n",
      ">60, c[0.693,50], d[0.692,0.568], g[0.887]\n",
      ">61, c[0.693,44], d[0.692,0.526], g[0.947]\n",
      ">62, c[0.693,48], d[0.692,0.554], g[0.911]\n",
      ">63, c[0.693,40], d[0.692,0.522], g[0.901]\n",
      ">64, c[0.693,62], d[0.692,0.549], g[0.879]\n",
      ">65, c[0.693,56], d[0.692,0.531], g[0.924]\n",
      ">66, c[0.693,52], d[0.692,0.539], g[0.905]\n",
      ">67, c[0.693,52], d[0.692,0.527], g[0.943]\n",
      ">68, c[0.694,30], d[0.692,0.520], g[0.966]\n",
      ">69, c[0.693,58], d[0.692,0.540], g[0.937]\n",
      ">70, c[0.693,44], d[0.692,0.544], g[0.934]\n",
      ">71, c[0.693,56], d[0.692,0.511], g[0.906]\n",
      ">72, c[0.693,48], d[0.692,0.544], g[0.973]\n",
      ">73, c[0.693,52], d[0.692,0.537], g[0.919]\n",
      ">74, c[0.693,56], d[0.692,0.528], g[0.980]\n",
      ">75, c[0.693,56], d[0.692,0.507], g[0.947]\n",
      ">76, c[0.694,32], d[0.692,0.527], g[0.960]\n",
      ">77, c[0.693,56], d[0.692,0.519], g[1.443]\n",
      ">78, c[0.693,52], d[0.692,0.565], g[4.077]\n",
      ">79, c[0.693,48], d[0.692,0.507], g[0.965]\n",
      ">80, c[0.693,64], d[0.692,0.482], g[1.012]\n",
      ">81, c[0.693,44], d[0.692,0.477], g[1.026]\n",
      ">82, c[0.693,38], d[0.692,0.436], g[1.075]\n",
      ">83, c[0.693,54], d[0.692,0.437], g[1.106]\n",
      ">84, c[0.693,58], d[0.692,0.402], g[1.084]\n",
      ">85, c[0.693,56], d[0.692,0.410], g[1.178]\n",
      ">86, c[0.693,52], d[0.692,0.352], g[1.201]\n",
      ">87, c[0.693,46], d[0.692,0.383], g[1.269]\n",
      ">88, c[0.693,58], d[0.692,0.389], g[1.321]\n",
      ">89, c[0.693,48], d[0.692,0.314], g[1.397]\n",
      ">90, c[0.693,42], d[0.692,0.324], g[1.374]\n",
      ">91, c[0.693,56], d[0.692,0.307], g[1.513]\n",
      ">92, c[0.693,44], d[0.691,0.308], g[1.450]\n",
      ">93, c[0.693,46], d[0.691,0.267], g[1.539]\n",
      ">94, c[0.693,44], d[0.691,0.254], g[1.652]\n",
      ">95, c[0.693,60], d[0.691,0.247], g[1.599]\n",
      ">96, c[0.693,54], d[0.691,0.275], g[1.688]\n",
      ">97, c[0.693,64], d[0.691,0.208], g[1.685]\n",
      ">98, c[0.694,36], d[0.691,0.205], g[1.782]\n",
      ">99, c[0.693,44], d[0.691,0.194], g[1.956]\n",
      ">100, c[0.693,54], d[0.691,0.194], g[1.956]\n",
      ">101, c[0.693,50], d[0.691,0.164], g[1.884]\n",
      ">102, c[0.693,44], d[0.691,0.178], g[2.063]\n",
      ">103, c[0.693,66], d[0.691,0.132], g[2.025]\n",
      ">104, c[0.693,50], d[0.691,0.159], g[2.090]\n",
      ">105, c[0.693,54], d[0.691,0.169], g[2.141]\n",
      ">106, c[0.693,50], d[0.691,0.174], g[2.216]\n",
      ">107, c[0.693,54], d[0.690,0.146], g[2.229]\n",
      ">108, c[0.693,58], d[0.690,0.128], g[2.478]\n",
      ">109, c[0.693,54], d[0.690,0.111], g[2.303]\n",
      ">110, c[0.693,48], d[0.690,0.259], g[2.370]\n",
      ">111, c[0.693,42], d[0.690,0.140], g[2.437]\n",
      ">112, c[0.693,60], d[0.690,0.095], g[2.560]\n",
      ">113, c[0.693,44], d[0.690,0.118], g[2.704]\n",
      ">114, c[0.693,48], d[0.690,0.116], g[2.495]\n",
      ">115, c[0.693,60], d[0.690,0.108], g[2.667]\n",
      ">116, c[0.694,38], d[0.690,0.120], g[2.643]\n",
      ">117, c[0.693,58], d[0.690,0.095], g[2.695]\n",
      ">118, c[0.693,64], d[0.690,0.084], g[2.948]\n",
      ">119, c[0.694,38], d[0.689,0.100], g[2.756]\n",
      ">120, c[0.693,50], d[0.689,0.078], g[2.791]\n",
      ">121, c[0.693,60], d[0.689,0.062], g[2.875]\n",
      ">122, c[0.694,40], d[0.689,0.097], g[2.869]\n",
      ">123, c[0.693,50], d[0.689,0.093], g[2.912]\n",
      ">124, c[0.693,44], d[0.689,0.067], g[2.994]\n",
      ">125, c[0.693,48], d[0.689,0.085], g[2.995]\n",
      ">126, c[0.693,50], d[0.689,0.076], g[2.911]\n",
      ">127, c[0.693,54], d[0.689,0.066], g[2.952]\n",
      ">128, c[0.693,52], d[0.689,0.072], g[2.998]\n",
      ">129, c[0.694,40], d[0.688,0.063], g[3.167]\n",
      ">130, c[0.694,38], d[0.688,0.073], g[3.210]\n",
      ">131, c[0.693,46], d[0.688,0.058], g[3.123]\n",
      ">132, c[0.693,48], d[0.688,0.072], g[3.140]\n",
      ">133, c[0.693,48], d[0.688,0.054], g[3.131]\n",
      ">134, c[0.693,46], d[0.688,0.072], g[3.209]\n",
      ">135, c[0.693,62], d[0.688,0.069], g[3.257]\n",
      ">136, c[0.693,46], d[0.688,0.059], g[3.252]\n",
      ">137, c[0.693,50], d[0.688,0.052], g[3.579]\n",
      ">138, c[0.693,38], d[0.688,0.066], g[3.234]\n",
      ">139, c[0.693,46], d[0.687,0.041], g[3.410]\n",
      ">140, c[0.693,44], d[0.687,0.052], g[3.120]\n",
      ">141, c[0.693,48], d[0.687,0.049], g[3.293]\n",
      ">142, c[0.693,68], d[0.687,0.057], g[3.552]\n",
      ">143, c[0.693,52], d[0.687,0.050], g[3.346]\n",
      ">144, c[0.693,42], d[0.687,0.041], g[3.413]\n",
      ">145, c[0.693,36], d[0.687,0.058], g[3.464]\n",
      ">146, c[0.693,56], d[0.687,0.042], g[3.430]\n",
      ">147, c[0.693,42], d[0.687,0.050], g[3.352]\n",
      ">148, c[0.693,36], d[0.687,0.046], g[3.520]\n",
      ">149, c[0.693,44], d[0.686,0.061], g[3.501]\n",
      ">150, c[0.693,46], d[0.686,0.041], g[3.522]\n",
      ">151, c[0.693,60], d[0.686,0.031], g[3.604]\n",
      ">152, c[0.693,50], d[0.686,0.039], g[3.544]\n",
      ">153, c[0.693,46], d[0.686,0.038], g[3.695]\n",
      ">154, c[0.693,40], d[0.686,0.054], g[3.511]\n",
      ">155, c[0.693,46], d[0.686,0.041], g[3.552]\n",
      ">156, c[0.693,40], d[0.686,0.034], g[3.718]\n",
      ">157, c[0.693,48], d[0.686,0.041], g[3.576]\n",
      ">158, c[0.693,46], d[0.685,0.031], g[3.634]\n",
      ">159, c[0.693,46], d[0.685,0.038], g[3.539]\n",
      ">160, c[0.693,58], d[0.685,0.034], g[3.672]\n",
      ">161, c[0.693,36], d[0.685,0.044], g[3.852]\n",
      ">162, c[0.693,48], d[0.685,0.034], g[3.845]\n",
      ">163, c[0.693,56], d[0.685,0.038], g[3.763]\n",
      ">164, c[0.693,62], d[0.685,0.037], g[3.946]\n",
      ">165, c[0.693,52], d[0.685,0.039], g[3.897]\n",
      ">166, c[0.693,58], d[0.685,0.033], g[3.719]\n",
      ">167, c[0.693,38], d[0.684,0.027], g[3.875]\n",
      ">168, c[0.693,44], d[0.684,0.026], g[3.785]\n",
      ">169, c[0.693,46], d[0.684,0.038], g[3.767]\n",
      ">170, c[0.693,48], d[0.684,0.022], g[3.906]\n",
      ">171, c[0.693,56], d[0.684,0.031], g[4.120]\n",
      ">172, c[0.693,50], d[0.684,0.039], g[3.928]\n",
      ">173, c[0.693,60], d[0.684,0.037], g[3.917]\n",
      ">174, c[0.693,54], d[0.684,0.030], g[3.939]\n",
      ">175, c[0.693,54], d[0.684,0.028], g[4.017]\n",
      ">176, c[0.693,46], d[0.683,0.033], g[3.874]\n",
      ">177, c[0.693,58], d[0.683,0.028], g[4.302]\n",
      ">178, c[0.693,46], d[0.683,0.031], g[3.713]\n",
      ">179, c[0.693,32], d[0.683,0.041], g[4.063]\n",
      ">180, c[0.693,50], d[0.683,0.028], g[4.021]\n",
      ">181, c[0.693,48], d[0.683,0.027], g[4.010]\n",
      ">182, c[0.693,54], d[0.683,0.032], g[3.867]\n",
      ">183, c[0.693,54], d[0.683,0.029], g[4.209]\n",
      ">184, c[0.693,52], d[0.683,0.030], g[4.206]\n",
      ">185, c[0.693,64], d[0.682,0.027], g[4.008]\n",
      ">186, c[0.693,50], d[0.682,0.028], g[4.087]\n",
      ">187, c[0.693,46], d[0.682,0.023], g[3.962]\n",
      ">188, c[0.693,52], d[0.682,0.028], g[4.205]\n",
      ">189, c[0.693,52], d[0.682,0.024], g[4.073]\n",
      ">190, c[0.693,46], d[0.682,0.035], g[4.142]\n",
      ">191, c[0.693,58], d[0.682,0.022], g[3.929]\n",
      ">192, c[0.693,58], d[0.682,0.029], g[4.175]\n",
      ">193, c[0.693,52], d[0.682,0.028], g[4.085]\n",
      ">194, c[0.693,42], d[0.681,0.019], g[4.249]\n",
      ">195, c[0.693,44], d[0.681,0.025], g[4.208]\n",
      ">196, c[0.693,44], d[0.681,0.027], g[4.148]\n",
      ">197, c[0.693,52], d[0.681,0.022], g[4.302]\n",
      ">198, c[0.693,48], d[0.681,0.019], g[4.339]\n",
      ">199, c[0.693,58], d[0.681,0.030], g[4.211]\n",
      ">200, c[0.693,32], d[0.681,0.024], g[4.338]\n",
      ">201, c[0.693,52], d[0.681,0.025], g[4.110]\n",
      ">202, c[0.693,64], d[0.680,0.026], g[4.328]\n",
      ">203, c[0.693,46], d[0.680,0.022], g[4.225]\n",
      ">204, c[0.693,52], d[0.680,0.027], g[4.461]\n",
      ">205, c[0.693,54], d[0.680,0.025], g[4.487]\n",
      ">206, c[0.693,56], d[0.680,0.015], g[4.378]\n",
      ">207, c[0.693,48], d[0.680,0.015], g[4.310]\n",
      ">208, c[0.693,60], d[0.680,0.022], g[4.264]\n",
      ">209, c[0.693,56], d[0.680,0.018], g[4.399]\n",
      ">210, c[0.693,60], d[0.680,0.021], g[4.310]\n",
      ">211, c[0.694,38], d[0.679,0.026], g[4.309]\n",
      ">212, c[0.693,56], d[0.679,0.020], g[4.353]\n",
      ">213, c[0.693,62], d[0.679,0.017], g[4.497]\n",
      ">214, c[0.693,50], d[0.679,0.023], g[4.389]\n",
      ">215, c[0.693,44], d[0.679,0.021], g[4.333]\n",
      ">216, c[0.693,40], d[0.679,0.023], g[4.448]\n",
      ">217, c[0.693,50], d[0.679,0.013], g[4.437]\n",
      ">218, c[0.693,42], d[0.679,0.023], g[4.493]\n",
      ">219, c[0.693,52], d[0.678,0.018], g[4.420]\n",
      ">220, c[0.693,52], d[0.678,0.016], g[4.608]\n",
      ">221, c[0.693,54], d[0.678,0.016], g[4.152]\n",
      ">222, c[0.693,40], d[0.678,0.018], g[4.546]\n",
      ">223, c[0.693,54], d[0.678,0.018], g[4.457]\n",
      ">224, c[0.693,60], d[0.678,0.013], g[4.440]\n",
      ">225, c[0.693,50], d[0.678,0.018], g[4.312]\n",
      ">226, c[0.693,52], d[0.678,0.020], g[4.511]\n",
      ">227, c[0.693,42], d[0.678,0.017], g[4.359]\n",
      ">228, c[0.693,52], d[0.677,0.015], g[4.470]\n",
      ">229, c[0.693,62], d[0.677,0.014], g[4.595]\n",
      ">230, c[0.693,62], d[0.677,0.016], g[6.110]\n",
      ">231, c[0.693,50], d[0.677,0.022], g[5.233]\n",
      ">232, c[0.693,48], d[0.677,0.012], g[5.155]\n",
      ">233, c[0.693,56], d[0.677,0.009], g[5.111]\n",
      ">234, c[0.693,60], d[0.677,0.007], g[6.094]\n",
      ">235, c[0.693,44], d[0.677,0.011], g[16.511]\n",
      ">236, c[0.694,38], d[0.676,0.019], g[5.366]\n",
      ">237, c[0.693,58], d[0.676,0.012], g[5.228]\n",
      ">238, c[0.693,58], d[0.676,0.015], g[5.150]\n",
      ">239, c[0.693,50], d[0.676,0.012], g[5.117]\n",
      ">240, c[0.693,60], d[0.676,0.013], g[5.351]\n",
      ">241, c[0.693,46], d[0.676,0.015], g[5.151]\n",
      ">242, c[0.693,50], d[0.676,0.014], g[5.085]\n",
      ">243, c[0.693,50], d[0.676,0.007], g[18.204]\n",
      ">244, c[0.693,56], d[0.676,0.009], g[5.047]\n",
      ">245, c[0.693,62], d[0.675,0.010], g[5.291]\n",
      ">246, c[0.693,46], d[0.675,0.012], g[6.001]\n",
      ">247, c[0.693,48], d[0.675,0.012], g[5.264]\n",
      ">248, c[0.693,46], d[0.675,0.014], g[5.414]\n",
      ">249, c[0.693,48], d[0.675,0.010], g[5.118]\n",
      ">250, c[0.693,48], d[0.675,0.011], g[5.042]\n",
      ">251, c[0.693,46], d[0.675,0.009], g[5.235]\n",
      ">252, c[0.693,46], d[0.675,0.010], g[5.071]\n",
      ">253, c[0.693,48], d[0.674,0.012], g[5.206]\n",
      ">254, c[0.693,50], d[0.674,0.007], g[5.278]\n",
      ">255, c[0.693,58], d[0.674,0.009], g[5.361]\n",
      ">256, c[0.693,52], d[0.674,0.012], g[5.005]\n",
      ">257, c[0.693,52], d[0.674,0.007], g[5.497]\n",
      ">258, c[0.694,38], d[0.674,0.012], g[5.452]\n",
      ">259, c[0.693,52], d[0.674,0.009], g[5.901]\n",
      ">260, c[0.693,50], d[0.674,0.008], g[5.346]\n",
      ">261, c[0.693,48], d[0.673,0.011], g[5.300]\n",
      ">262, c[0.693,50], d[0.673,0.009], g[5.563]\n",
      ">263, c[0.693,54], d[0.673,0.011], g[5.392]\n",
      ">264, c[0.694,40], d[0.673,0.008], g[5.349]\n",
      ">265, c[0.693,54], d[0.673,0.008], g[5.505]\n",
      ">266, c[0.693,54], d[0.673,0.008], g[5.452]\n",
      ">267, c[0.693,46], d[0.673,0.011], g[5.361]\n",
      ">268, c[0.694,42], d[0.673,0.006], g[5.660]\n",
      ">269, c[0.693,62], d[0.672,0.008], g[5.438]\n",
      ">270, c[0.693,48], d[0.672,0.007], g[5.408]\n",
      ">271, c[0.693,46], d[0.672,0.009], g[5.350]\n",
      ">272, c[0.693,46], d[0.672,0.007], g[5.551]\n",
      ">273, c[0.692,66], d[0.672,0.007], g[5.359]\n",
      ">274, c[0.693,48], d[0.672,0.009], g[5.410]\n",
      ">275, c[0.694,32], d[0.672,0.005], g[5.275]\n",
      ">276, c[0.693,50], d[0.672,0.006], g[5.444]\n",
      ">277, c[0.693,60], d[0.672,0.007], g[5.734]\n",
      ">278, c[0.693,42], d[0.671,0.007], g[5.924]\n",
      ">279, c[0.693,52], d[0.671,0.006], g[7.054]\n",
      ">280, c[0.693,46], d[0.671,0.007], g[5.402]\n",
      ">281, c[0.693,54], d[0.671,0.007], g[5.452]\n",
      ">282, c[0.693,48], d[0.671,0.006], g[5.984]\n",
      ">283, c[0.693,58], d[0.671,0.006], g[5.591]\n",
      ">284, c[0.693,50], d[0.671,0.011], g[5.813]\n",
      ">285, c[0.693,46], d[0.671,0.006], g[5.609]\n",
      ">286, c[0.693,58], d[0.670,0.009], g[7.538]\n",
      ">287, c[0.693,46], d[0.670,0.009], g[5.646]\n",
      ">288, c[0.693,64], d[0.670,0.008], g[5.608]\n",
      ">289, c[0.693,52], d[0.670,0.006], g[6.035]\n",
      ">290, c[0.693,54], d[0.670,0.008], g[5.740]\n",
      ">291, c[0.694,40], d[0.670,0.010], g[5.674]\n",
      ">292, c[0.693,48], d[0.670,0.006], g[5.642]\n",
      ">293, c[0.693,48], d[0.670,0.006], g[12.363]\n",
      ">294, c[0.693,48], d[0.669,0.004], g[18.541]\n",
      ">295, c[0.693,44], d[0.669,0.006], g[6.345]\n",
      ">296, c[0.693,58], d[0.669,0.005], g[7.294]\n",
      ">297, c[0.693,58], d[0.669,0.007], g[6.698]\n",
      ">298, c[0.693,46], d[0.669,0.007], g[20.357]\n",
      ">299, c[0.694,42], d[0.669,0.007], g[21.496]\n",
      ">300, c[0.693,56], d[0.669,0.008], g[20.213]\n",
      ">301, c[0.693,58], d[0.669,0.006], g[6.298]\n",
      ">302, c[0.693,60], d[0.668,0.009], g[6.214]\n",
      ">303, c[0.693,44], d[0.668,0.006], g[5.626]\n",
      ">304, c[0.693,50], d[0.668,0.008], g[24.197]\n",
      ">305, c[0.694,42], d[0.668,0.007], g[5.413]\n",
      ">306, c[0.693,44], d[0.668,0.013], g[5.184]\n",
      ">307, c[0.693,44], d[0.668,0.006], g[5.226]\n",
      ">308, c[0.693,46], d[0.668,0.009], g[5.124]\n",
      ">309, c[0.693,44], d[0.668,0.009], g[4.947]\n",
      ">310, c[0.693,44], d[0.667,0.010], g[5.051]\n",
      ">311, c[0.693,46], d[0.667,0.010], g[5.393]\n",
      ">312, c[0.693,50], d[0.667,0.012], g[5.174]\n",
      ">313, c[0.693,58], d[0.667,0.008], g[5.198]\n",
      ">314, c[0.693,54], d[0.667,0.010], g[4.947]\n",
      ">315, c[0.693,50], d[0.667,0.010], g[5.334]\n",
      ">316, c[0.693,46], d[0.667,0.007], g[5.076]\n",
      ">317, c[0.693,52], d[0.667,0.009], g[5.301]\n",
      ">318, c[0.693,62], d[0.666,0.009], g[5.048]\n",
      ">319, c[0.693,56], d[0.666,0.011], g[5.272]\n",
      ">320, c[0.693,62], d[0.666,0.006], g[5.255]\n",
      ">321, c[0.693,52], d[0.666,0.012], g[5.191]\n",
      ">322, c[0.693,52], d[0.666,0.017], g[5.216]\n",
      ">323, c[0.693,52], d[0.666,0.007], g[5.300]\n",
      ">324, c[0.693,60], d[0.666,0.007], g[5.411]\n",
      ">325, c[0.693,46], d[0.666,0.005], g[5.331]\n",
      ">326, c[0.693,50], d[0.665,0.009], g[5.377]\n",
      ">327, c[0.693,54], d[0.665,0.010], g[5.307]\n",
      ">328, c[0.693,50], d[0.665,0.012], g[5.330]\n",
      ">329, c[0.693,50], d[0.665,0.009], g[5.133]\n",
      ">330, c[0.693,50], d[0.665,0.006], g[5.295]\n",
      ">331, c[0.692,70], d[0.665,0.006], g[5.258]\n",
      ">332, c[0.693,58], d[0.665,0.009], g[5.594]\n",
      ">333, c[0.693,54], d[0.665,0.009], g[5.418]\n",
      ">334, c[0.693,46], d[0.664,0.009], g[5.397]\n",
      ">335, c[0.693,54], d[0.664,0.007], g[5.402]\n",
      ">336, c[0.693,52], d[0.664,0.008], g[5.454]\n",
      ">337, c[0.694,42], d[0.664,0.007], g[5.228]\n",
      ">338, c[0.694,34], d[0.664,0.008], g[5.412]\n",
      ">339, c[0.694,44], d[0.664,0.008], g[5.481]\n",
      ">340, c[0.693,46], d[0.664,0.008], g[5.449]\n",
      ">341, c[0.693,54], d[0.664,0.006], g[5.378]\n",
      ">342, c[0.694,44], d[0.664,0.008], g[5.389]\n",
      ">343, c[0.694,38], d[0.663,0.010], g[5.282]\n",
      ">344, c[0.693,48], d[0.663,0.006], g[5.736]\n",
      ">345, c[0.693,48], d[0.663,0.005], g[5.220]\n",
      ">346, c[0.694,42], d[0.663,0.007], g[5.464]\n",
      ">347, c[0.692,64], d[0.663,0.006], g[5.399]\n",
      ">348, c[0.693,50], d[0.663,0.005], g[5.393]\n",
      ">349, c[0.693,46], d[0.663,0.009], g[5.408]\n",
      ">350, c[0.693,56], d[0.663,0.007], g[5.450]\n",
      ">351, c[0.693,56], d[0.662,0.006], g[5.757]\n",
      ">352, c[0.693,52], d[0.662,0.006], g[5.585]\n",
      ">353, c[0.693,56], d[0.662,0.007], g[5.555]\n",
      ">354, c[0.693,50], d[0.662,0.006], g[5.660]\n",
      ">355, c[0.693,48], d[0.662,0.010], g[5.626]\n",
      ">356, c[0.693,56], d[0.662,0.006], g[5.772]\n",
      ">357, c[0.693,50], d[0.662,0.005], g[5.606]\n",
      ">358, c[0.694,44], d[0.662,0.008], g[5.516]\n",
      ">359, c[0.693,54], d[0.661,0.010], g[5.642]\n",
      ">360, c[0.694,44], d[0.661,0.005], g[5.517]\n",
      ">361, c[0.693,50], d[0.661,0.005], g[5.569]\n",
      ">362, c[0.692,62], d[0.661,0.009], g[5.768]\n",
      ">363, c[0.693,50], d[0.661,0.007], g[5.542]\n",
      ">364, c[0.693,54], d[0.661,0.005], g[5.577]\n",
      ">365, c[0.694,34], d[0.661,0.007], g[5.543]\n",
      ">366, c[0.693,54], d[0.661,0.008], g[5.629]\n",
      ">367, c[0.693,60], d[0.660,0.005], g[5.715]\n",
      ">368, c[0.693,46], d[0.660,0.007], g[5.597]\n",
      ">369, c[0.694,44], d[0.660,0.007], g[5.650]\n",
      ">370, c[0.693,54], d[0.660,0.006], g[5.541]\n",
      ">371, c[0.694,44], d[0.660,0.007], g[5.636]\n",
      ">372, c[0.694,44], d[0.660,0.004], g[5.537]\n",
      ">373, c[0.693,52], d[0.660,0.006], g[5.924]\n",
      ">374, c[0.693,46], d[0.660,0.006], g[5.639]\n",
      ">375, c[0.693,48], d[0.659,0.006], g[5.751]\n",
      ">376, c[0.693,54], d[0.659,0.006], g[5.908]\n",
      ">377, c[0.693,46], d[0.659,0.006], g[5.861]\n",
      ">378, c[0.693,44], d[0.659,0.006], g[5.896]\n",
      ">379, c[0.693,50], d[0.659,0.010], g[5.990]\n",
      ">380, c[0.693,56], d[0.659,0.005], g[5.597]\n",
      ">381, c[0.693,50], d[0.659,0.006], g[5.721]\n",
      ">382, c[0.694,40], d[0.659,0.008], g[5.759]\n",
      ">383, c[0.693,50], d[0.658,0.004], g[5.939]\n",
      ">384, c[0.693,50], d[0.658,0.005], g[5.762]\n",
      ">385, c[0.693,52], d[0.658,0.005], g[5.799]\n",
      ">386, c[0.693,54], d[0.658,0.005], g[5.847]\n",
      ">387, c[0.693,58], d[0.658,0.005], g[5.952]\n",
      ">388, c[0.693,48], d[0.658,0.005], g[5.749]\n",
      ">389, c[0.693,56], d[0.658,0.006], g[5.869]\n",
      ">390, c[0.694,42], d[0.658,0.006], g[5.853]\n",
      ">391, c[0.693,46], d[0.657,0.005], g[5.877]\n",
      ">392, c[0.692,66], d[0.657,0.005], g[5.926]\n",
      ">393, c[0.693,48], d[0.657,0.005], g[5.909]\n",
      ">394, c[0.692,62], d[0.657,0.004], g[5.781]\n",
      ">395, c[0.694,36], d[0.657,0.005], g[6.206]\n",
      ">396, c[0.693,48], d[0.657,0.004], g[5.793]\n",
      ">397, c[0.692,70], d[0.657,0.004], g[5.740]\n",
      ">398, c[0.694,38], d[0.657,0.006], g[6.010]\n",
      ">399, c[0.693,54], d[0.656,0.004], g[6.102]\n",
      ">400, c[0.693,48], d[0.656,0.006], g[6.028]\n",
      ">401, c[0.692,64], d[0.656,0.007], g[6.157]\n",
      ">402, c[0.693,54], d[0.656,0.003], g[6.050]\n",
      ">403, c[0.694,40], d[0.656,0.004], g[5.866]\n",
      ">404, c[0.694,40], d[0.656,0.004], g[5.813]\n",
      ">405, c[0.693,58], d[0.656,0.004], g[6.310]\n",
      ">406, c[0.694,40], d[0.656,0.005], g[5.913]\n",
      ">407, c[0.694,40], d[0.655,0.005], g[6.131]\n",
      ">408, c[0.693,50], d[0.655,0.005], g[5.919]\n",
      ">409, c[0.693,50], d[0.655,0.005], g[6.069]\n",
      ">410, c[0.693,48], d[0.655,0.005], g[6.020]\n",
      ">411, c[0.693,54], d[0.655,0.004], g[5.858]\n",
      ">412, c[0.693,46], d[0.655,0.004], g[6.107]\n",
      ">413, c[0.693,58], d[0.655,0.003], g[5.898]\n",
      ">414, c[0.693,46], d[0.655,0.003], g[6.098]\n",
      ">415, c[0.693,56], d[0.654,0.005], g[6.242]\n",
      ">416, c[0.693,52], d[0.654,0.004], g[6.228]\n",
      ">417, c[0.693,48], d[0.654,0.004], g[6.200]\n",
      ">418, c[0.693,58], d[0.654,0.003], g[6.046]\n",
      ">419, c[0.694,34], d[0.654,0.005], g[6.197]\n",
      ">420, c[0.693,54], d[0.654,0.005], g[6.168]\n",
      ">421, c[0.693,48], d[0.654,0.003], g[6.073]\n",
      ">422, c[0.693,50], d[0.654,0.005], g[6.103]\n",
      ">423, c[0.692,68], d[0.653,0.004], g[6.158]\n",
      ">424, c[0.693,48], d[0.653,0.007], g[6.324]\n",
      ">425, c[0.694,36], d[0.653,0.004], g[6.183]\n",
      ">426, c[0.693,48], d[0.653,0.003], g[5.998]\n",
      ">427, c[0.693,58], d[0.653,0.005], g[6.272]\n",
      ">428, c[0.693,62], d[0.653,0.003], g[6.261]\n",
      ">429, c[0.693,56], d[0.653,0.003], g[6.268]\n",
      ">430, c[0.693,46], d[0.653,0.004], g[5.971]\n",
      ">431, c[0.693,56], d[0.652,0.005], g[6.266]\n",
      ">432, c[0.694,42], d[0.652,0.005], g[6.237]\n",
      ">433, c[0.693,58], d[0.652,0.004], g[6.398]\n",
      ">434, c[0.694,44], d[0.652,0.004], g[6.124]\n",
      ">435, c[0.693,54], d[0.652,0.005], g[6.361]\n",
      ">436, c[0.693,52], d[0.652,0.003], g[6.259]\n",
      ">437, c[0.694,44], d[0.652,0.004], g[6.311]\n",
      ">438, c[0.693,54], d[0.652,0.003], g[6.371]\n",
      ">439, c[0.693,50], d[0.651,0.006], g[6.245]\n",
      ">440, c[0.694,42], d[0.651,0.003], g[6.304]\n",
      ">441, c[0.694,40], d[0.651,0.004], g[6.244]\n",
      ">442, c[0.692,62], d[0.651,0.004], g[6.165]\n",
      ">443, c[0.693,48], d[0.651,0.003], g[6.460]\n",
      ">444, c[0.694,38], d[0.651,0.004], g[6.355]\n",
      ">445, c[0.693,52], d[0.651,0.003], g[6.267]\n",
      ">446, c[0.693,46], d[0.651,0.003], g[6.311]\n",
      ">447, c[0.693,60], d[0.650,0.004], g[6.103]\n",
      ">448, c[0.693,60], d[0.650,0.005], g[6.199]\n",
      ">449, c[0.694,40], d[0.650,0.005], g[6.516]\n",
      ">450, c[0.693,46], d[0.650,0.004], g[6.178]\n",
      ">451, c[0.693,48], d[0.650,0.004], g[6.372]\n",
      ">452, c[0.693,54], d[0.650,0.005], g[6.338]\n",
      ">453, c[0.693,58], d[0.650,0.003], g[6.310]\n",
      ">454, c[0.693,48], d[0.650,0.003], g[6.461]\n",
      ">455, c[0.692,66], d[0.649,0.004], g[6.460]\n",
      ">456, c[0.693,48], d[0.649,0.004], g[6.403]\n",
      ">457, c[0.694,40], d[0.649,0.004], g[6.349]\n",
      ">458, c[0.693,60], d[0.649,0.005], g[6.541]\n",
      ">459, c[0.693,52], d[0.649,0.002], g[6.501]\n",
      ">460, c[0.694,40], d[0.649,0.003], g[6.286]\n",
      ">461, c[0.693,58], d[0.649,0.003], g[6.587]\n",
      ">462, c[0.694,42], d[0.649,0.003], g[6.323]\n",
      ">463, c[0.694,44], d[0.648,0.003], g[6.169]\n",
      ">464, c[0.693,48], d[0.648,0.004], g[6.529]\n",
      ">465, c[0.693,54], d[0.648,0.004], g[6.714]\n",
      ">466, c[0.693,56], d[0.648,0.002], g[6.346]\n",
      ">467, c[0.694,42], d[0.648,0.003], g[6.364]\n",
      ">468, c[0.693,46], d[0.648,0.004], g[6.507]\n",
      ">469, c[0.693,44], d[0.648,0.003], g[6.175]\n",
      ">470, c[0.693,54], d[0.648,0.003], g[6.524]\n",
      ">471, c[0.693,50], d[0.647,0.004], g[6.635]\n",
      ">472, c[0.693,54], d[0.647,0.003], g[6.265]\n",
      ">473, c[0.693,50], d[0.647,0.003], g[6.314]\n",
      ">474, c[0.693,58], d[0.647,0.002], g[6.669]\n",
      ">475, c[0.693,56], d[0.647,0.003], g[6.539]\n",
      ">476, c[0.693,52], d[0.647,0.002], g[6.452]\n",
      ">477, c[0.693,58], d[0.647,0.004], g[6.696]\n",
      ">478, c[0.693,50], d[0.647,0.003], g[6.601]\n",
      ">479, c[0.694,42], d[0.646,0.003], g[6.356]\n",
      ">480, c[0.692,64], d[0.646,0.004], g[6.492]\n",
      ">481, c[0.693,46], d[0.646,0.003], g[6.600]\n",
      ">482, c[0.693,58], d[0.646,0.004], g[6.613]\n",
      ">483, c[0.693,52], d[0.646,0.005], g[6.748]\n",
      ">484, c[0.694,40], d[0.646,0.003], g[6.487]\n",
      ">485, c[0.694,42], d[0.646,0.002], g[6.597]\n",
      ">486, c[0.693,52], d[0.646,0.006], g[6.505]\n",
      ">487, c[0.694,42], d[0.645,0.004], g[6.719]\n",
      ">488, c[0.693,54], d[0.645,0.003], g[6.473]\n",
      ">489, c[0.693,60], d[0.645,0.002], g[6.623]\n",
      ">490, c[0.694,38], d[0.645,0.003], g[6.550]\n",
      ">491, c[0.694,44], d[0.645,0.003], g[6.765]\n",
      ">492, c[0.693,52], d[0.645,0.003], g[6.476]\n",
      ">493, c[0.693,46], d[0.645,0.003], g[6.730]\n",
      ">494, c[0.693,46], d[0.645,0.003], g[6.558]\n",
      ">495, c[0.694,42], d[0.644,0.003], g[6.471]\n",
      ">496, c[0.693,54], d[0.644,0.003], g[6.849]\n",
      ">497, c[0.693,50], d[0.644,0.003], g[6.631]\n",
      ">498, c[0.693,60], d[0.644,0.004], g[6.840]\n",
      ">499, c[0.693,44], d[0.644,0.003], g[6.861]\n",
      ">500, c[0.693,58], d[0.644,0.002], g[6.430]\n",
      ">501, c[0.693,52], d[0.644,0.003], g[6.664]\n",
      ">502, c[0.693,44], d[0.644,0.003], g[6.568]\n",
      ">503, c[0.693,46], d[0.644,0.003], g[6.799]\n",
      ">504, c[0.693,58], d[0.643,0.002], g[6.699]\n",
      ">505, c[0.694,42], d[0.643,0.004], g[6.634]\n",
      ">506, c[0.693,54], d[0.643,0.002], g[6.823]\n",
      ">507, c[0.693,60], d[0.643,0.003], g[6.688]\n",
      ">508, c[0.693,54], d[0.643,0.004], g[6.638]\n",
      ">509, c[0.693,50], d[0.643,0.002], g[6.563]\n",
      ">510, c[0.694,38], d[0.643,0.002], g[6.497]\n",
      ">511, c[0.693,56], d[0.643,0.002], g[6.774]\n",
      ">512, c[0.693,52], d[0.642,0.004], g[6.835]\n",
      ">513, c[0.693,54], d[0.642,0.003], g[6.746]\n",
      ">514, c[0.692,62], d[0.642,0.003], g[6.919]\n",
      ">515, c[0.693,56], d[0.642,0.005], g[6.729]\n",
      ">516, c[0.693,46], d[0.642,0.004], g[6.884]\n",
      ">517, c[0.693,52], d[0.642,0.003], g[6.755]\n",
      ">518, c[0.694,42], d[0.642,0.002], g[6.922]\n",
      ">519, c[0.693,56], d[0.642,0.002], g[6.623]\n",
      ">520, c[0.693,48], d[0.641,0.002], g[7.010]\n",
      ">521, c[0.693,46], d[0.641,0.003], g[6.567]\n",
      ">522, c[0.693,52], d[0.641,0.003], g[6.893]\n",
      ">523, c[0.693,54], d[0.641,0.003], g[6.900]\n",
      ">524, c[0.694,44], d[0.641,0.005], g[6.767]\n",
      ">525, c[0.693,54], d[0.641,0.002], g[6.868]\n",
      ">526, c[0.693,52], d[0.641,0.003], g[6.913]\n",
      ">527, c[0.693,52], d[0.641,0.004], g[6.769]\n",
      ">528, c[0.693,50], d[0.640,0.002], g[6.796]\n",
      ">529, c[0.693,50], d[0.640,0.003], g[6.738]\n",
      ">530, c[0.693,56], d[0.640,0.002], g[6.890]\n",
      ">531, c[0.693,54], d[0.640,0.002], g[6.746]\n",
      ">532, c[0.693,56], d[0.640,0.002], g[6.878]\n",
      ">533, c[0.694,42], d[0.640,0.002], g[6.723]\n",
      ">534, c[0.694,42], d[0.640,0.004], g[6.807]\n",
      ">535, c[0.693,52], d[0.640,0.001], g[6.618]\n",
      ">536, c[0.693,58], d[0.639,0.002], g[6.707]\n",
      ">537, c[0.693,58], d[0.639,0.002], g[6.791]\n",
      ">538, c[0.693,54], d[0.639,0.002], g[6.838]\n",
      ">539, c[0.693,50], d[0.639,0.001], g[6.813]\n",
      ">540, c[0.694,38], d[0.639,0.003], g[6.703]\n"
     ]
    }
   ],
   "source": [
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "\n",
    "# create the discriminator models\n",
    "d_model, c_model = define_discriminator()\n",
    "\n",
    "# create the generator\n",
    "g_model = define_generator()\n",
    "\n",
    "# create the gan\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "\n",
    "# # load image data\n",
    "dataset = [X_train, y_train]\n",
    "\n",
    "# # train model\n",
    "train(g_model, d_model, c_model, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
