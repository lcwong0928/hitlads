{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from orion.data import load_signal\n",
    "from orion import Orion\n",
    "from orion.data import load_anomalies\n",
    "\n",
    "from mlprimitives.custom.timeseries_preprocessing import time_segments_aggregate\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from mlprimitives.custom.timeseries_preprocessing import rolling_window_sequences\n",
    "from orion.primitives.timeseries_preprocessing import slice_array_by_dims\n",
    "from mlprimitives import load_primitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers  # suppress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "input - timestamp, values\n",
    "\n",
    "output - start, end anomalous intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2818, 26), (7331, 26))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = load_signal('multivariate/S-1-train')\n",
    "X_test = load_signal('multivariate/S-1-test')\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2818, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates an equi-spaced time series by aggregating values over fixed specified interval\n",
    "params = {\"time_column\": \"timestamp\", \"interval\": 21600, \"method\": \"mean\"}\n",
    "primitive = load_primitive('mlprimitives.custom.timeseries_preprocessing.time_segments_aggregate', \n",
    "                           arguments=params)\n",
    "X, index = primitive.produce(X=X_train)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2818, 25)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This primitive is an imputation transformer for filling missing values\n",
    "params = {'X': X}\n",
    "primitive = load_primitive('sklearn.impute.SimpleImputer', \n",
    "                           arguments=params)\n",
    "primitive.fit()\n",
    "X = primitive.produce(X=X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2818, 25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This primitive transforms features by scaling each feature to a given range\n",
    "params = {\"feature_range\": [-1, 1], 'X': X}\n",
    "primitive = load_primitive('sklearn.preprocessing.MinMaxScaler', \n",
    "                           arguments=params)\n",
    "primitive.fit()\n",
    "X = primitive.produce(X=X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2718, 100, 25), (2718, 1), (2718,), (2718,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uses a rolling window approach to create the sub-sequences out of time series data\n",
    "params = {\"target_column\": 0, \"window_size\": 100, 'target_size': 1, 'step_size': 1}\n",
    "primitive = load_primitive('mlprimitives.custom.timeseries_preprocessing.rolling_window_sequences',\n",
    "                           arguments=params)\n",
    "X, y, index, target_index = primitive.produce(X=X, index=index)\n",
    "\n",
    "X.shape, y.shape, index.shape, target_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2718, 100, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target\n",
    "params = {\"target_index\": 0, \"axis\": 2}\n",
    "primitive = load_primitive('orion.primitives.timeseries_preprocessing.slice_array_by_dims',\n",
    "                           arguments=params)\n",
    "y = primitive.produce(X=X)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('y', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/lcwong/opt/anaconda3/envs/orion-env/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "2022-01-14 12:37:24.849986: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-01-14 12:37:24.867871: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff5d9827740 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-01-14 12:37:24.867886: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "/Users/lcwong/opt/anaconda3/envs/orion-env/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/Users/lcwong/opt/anaconda3/envs/orion-env/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "2022-01-14 12:37:36.358333: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.\n",
      "2022-01-14 12:37:36.379720: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.\n",
      "2022-01-14 12:37:36.450148: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:533] model_pruner failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'loss_2/model_2_loss/mean_squared_error/weighted_loss/concat' has self cycle fanin 'loss_2/model_2_loss/mean_squared_error/weighted_loss/concat'.\n",
      "2022-01-14 12:37:36.612990: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:533] remapper failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'loss_2/model_2_loss/mean_squared_error/weighted_loss/concat' has self cycle fanin 'loss_2/model_2_loss/mean_squared_error/weighted_loss/concat'.\n",
      "2022-01-14 12:37:36.635833: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:533] arithmetic_optimizer failed: Invalid argument: The graph couldn't be sorted in topological order.\n",
      "2022-01-14 12:37:36.653314: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.\n",
      "2022-01-14 12:37:36.673188: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.\n",
      "/Users/lcwong/opt/anaconda3/envs/orion-env/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1, [Dx loss: [-3.4915893  -7.0443277   0.50559795  0.30471402]] [Dz loss: [8.969591   0.14791751 7.969346   0.08523281]] [G loss: [-0.94994646 -0.45037007 -6.5507874   0.60512114]]\n"
     ]
    }
   ],
   "source": [
    "# this is a reconstruction model, namely Generative Adversarial Networks (GAN)\n",
    "params = {\"epochs\": 1, \"input_shape\":[100, 25], \"target_shape\": [100, 1]}\n",
    "\n",
    "primitive = load_primitive('orion.primitives.tadgan.TadGAN', \n",
    "                           arguments=params)\n",
    "primitive.fit(X=X, y=y)\n",
    "y_hat, critic = primitive.produce(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2718, 100, 1), (2718, 1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape, critic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, maxlen, embed_dim = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead)\n",
    "  but it must be broadcastable for addition.\n",
    "\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        \n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding, dropout_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # position encoding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                             input_vocab_size, pe_input, rate)\n",
    "\n",
    "    # self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "    #                        target_vocab_size, pe_target, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    # Keras models prefer if you pass all your inputs in the first argument\n",
    "    inp, tar = inputs\n",
    "\n",
    "    enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "    return final_output, attention_weights\n",
    "\n",
    "  def create_masks(self, inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by\n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, look_ahead_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1), Dimension(100), Dimension(1)])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layers = 1\n",
    "d_model = 25\n",
    "dff = 100\n",
    "num_heads = 5\n",
    "dropout_rate = 0.1\n",
    "maximum_position_encoding = 100\n",
    "\n",
    "sample_encoder = Encoder(num_layers=num_layers, \n",
    "                         d_model=d_model, \n",
    "                         num_heads=num_heads, \n",
    "                         dff=dff, \n",
    "                         maximum_position_encoding=maximum_position_encoding,\n",
    "                         dropout_rate=dropout_rate,\n",
    "                        )\n",
    "final_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "\n",
    "temp_input = tf.random.uniform((1, 100, 25), dtype=tf.float32, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "final_layer(sample_encoder_output).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Encoder(num_layers=num_layers, \n",
    "                         d_model=d_model, \n",
    "                         num_heads=num_heads, \n",
    "                         dff=dff, \n",
    "                         maximum_position_encoding=maximum_position_encoding,\n",
    "                         dropout_rate=dropout_rate,\n",
    "                        ))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generator()\n",
    "\n",
    "n_samples = 10\n",
    "samples = X[:n_samples]\n",
    "Xf = model.predict(samples)\n",
    "yf = tf.zeros((n_samples, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100, 1)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(10), Dimension(1)])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 5  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "# inputs = layers.Input(shape=(maxlen,embed_dim))\n",
    "inputs = tf.convert_to_tensor(X[:1], dtype=tf.float32)\n",
    "# embedding_layer = PositionEmbedding(maxlen, embed_dim)\n",
    "# x = embedding_layer(inputs)\n",
    "# transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "# x = transformer_block(x, True)\n",
    "# x = layers.GlobalAveragePooling1D()(x)\n",
    "# x = layers.Dropout(0.1)(x)\n",
    "# x = layers.Dense(20, activation=\"relu\")(x)\n",
    "# x = layers.Dropout(0.1)(x)\n",
    "# outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "# model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 1)         6273      \n",
      "=================================================================\n",
      "Total params: 1,164,289\n",
      "Trainable params: 1,164,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_5/f7yzqnxs6694l3c_yk9mjpzr0000gn/T/ipykernel_63938/1920197105.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# plot the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'generator_plot.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/orion-env/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir, expand_nested, dpi)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \"\"\"\n\u001b[1;32m    239\u001b[0m     dot = model_to_dot(model, show_shapes, show_layer_names, rankdir,\n\u001b[0;32m--> 240\u001b[0;31m                        expand_nested, dpi)\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/orion-env/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msubgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dashed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/orion-env/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         raise ImportError(\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "# example of defining the generator model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.utils.vis_utils import plot_model\n",
    " \n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 7x7 image\n",
    "    n_nodes = 128 * 7 * 7\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    # upsample to 14x14\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 28x28\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))\n",
    "    return model\n",
    " \n",
    "# define the size of the latent space\n",
    "latent_dim = 100\n",
    "# define the generator model\n",
    "model = define_generator(latent_dim)\n",
    "# summarize the model\n",
    "model.summary()\n",
    "# plot the model\n",
    "plot_model(model, to_file='generator_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MADGAN\n",
    "https://github.com/arunppsg/TadGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_path, signal_shape=100):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.signal_shape = signal_shape\n",
    "        self.lstm = nn.LSTM(input_size=self.signal_shape, hidden_size=20, num_layers=1, bidirectional=True)\n",
    "        self.dense = nn.Linear(in_features=40, out_features=20)\n",
    "        self.encoder_path = encoder_path\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(1, 64, self.signal_shape).float()\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "        x = self.dense(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_path, signal_shape=100):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.signal_shape = signal_shape\n",
    "        self.lstm = nn.LSTM(input_size=20, hidden_size=64, num_layers=2, bidirectional=True)\n",
    "        self.dense = nn.Linear(in_features=128, out_features=self.signal_shape)\n",
    "        self.decoder_path = decoder_path\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "        x = self.dense(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticX(nn.Module):\n",
    "    def __init__(self, critic_x_path, signal_shape=100):\n",
    "        super(CriticX, self).__init__()\n",
    "        self.signal_shape = signal_shape\n",
    "        self.dense1 = nn.Linear(in_features=self.signal_shape, out_features=20)\n",
    "        self.dense2 = nn.Linear(in_features=20, out_features=1)\n",
    "        self.critic_x_path = critic_x_path\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(1, 64, self.signal_shape).float()\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticZ(nn.Module):\n",
    "    def __init__(self, critic_z_path):\n",
    "        super(CriticZ, self).__init__()\n",
    "        self.dense1 = nn.Linear(in_features=20, out_features=1)\n",
    "        self.critic_z_path = critic_z_path\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        return (x)\n",
    "\n",
    "def unroll_signal(self, x):\n",
    "    x = np.array(x).reshape(100)\n",
    "    return np.median(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self):\n",
    "    \"\"\"\n",
    "    Returns a dataframe with original value, reconstructed value, reconstruction error, critic score\n",
    "    \"\"\"\n",
    "    df = self.test_dataset.copy()\n",
    "    X_ = list()\n",
    "\n",
    "    RE = list()  #Reconstruction error\n",
    "    CS = list()  #Critic score\n",
    "\n",
    "    for i in range(0, df.shape[0]):\n",
    "        x = df.rolled_signal[i]\n",
    "        x = tf.reshape(x, (1, 100, 1))\n",
    "        z = encoder(x)\n",
    "        z = tf.expand_dims(z, axis=2)\n",
    "        x_ = decoder(z)\n",
    "\n",
    "        re = dtw_reconstruction_error(tf.squeeze(x_).numpy(), tf.squeeze(x).numpy()) #reconstruction error\n",
    "        cs = critic_x(x)\n",
    "        cs = tf.squeeze(cs).numpy()\n",
    "        RE.append(re)\n",
    "        CS.append(cs)\n",
    "\n",
    "        x_ = unroll_signal(x_)\n",
    "\n",
    "        X_.append(x_)\n",
    "\n",
    "    df['generated_signals'] = X_\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-0be82e16c05b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manomaly_detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import model\n",
    "import anomaly_detection\n",
    "\n",
    "logging.basicConfig(filename='train.log', level=logging.DEBUG)\n",
    "\n",
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.signal_df = pd.read_csv(path)\n",
    "        self.signal_columns = self.make_signal_list()\n",
    "        self.make_rolling_signals()\n",
    "\n",
    "    def make_signal_list(self):\n",
    "        signal_list = list()\n",
    "        for i in range(-50, 50):\n",
    "            signal_list.append('signal'+str(i))\n",
    "        return signal_list\n",
    "\n",
    "    def make_rolling_signals(self):\n",
    "        for i in range(-50, 50):\n",
    "            self.signal_df['signal'+str(i)] = self.signal_df['signal'].shift(i)\n",
    "        self.signal_df = self.signal_df.dropna()\n",
    "        self.signal_df = self.signal_df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signal_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.signal_df.loc[idx]\n",
    "        x = row[self.signal_columns].values.astype(float)\n",
    "        x = torch.from_numpy(x)\n",
    "        return {'signal':x, 'anomaly':row['anomaly']}\n",
    "\n",
    "def critic_x_iteration(sample):\n",
    "    optim_cx.zero_grad()\n",
    "\n",
    "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
    "    valid_x = critic_x(x)\n",
    "    valid_x = torch.squeeze(valid_x)\n",
    "    critic_score_valid_x = torch.mean(torch.ones(valid_x.shape) * valid_x) #Wasserstein Loss\n",
    "\n",
    "    #The sampled z are the anomalous points - points deviating from actual distribution of z (obtained through encoding x)\n",
    "    z = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
    "    x_ = decoder(z)\n",
    "    fake_x = critic_x(x_)\n",
    "    fake_x = torch.squeeze(fake_x)\n",
    "    critic_score_fake_x = torch.mean(torch.ones(fake_x.shape) * fake_x)  #Wasserstein Loss\n",
    "\n",
    "    alpha = torch.rand(x.shape)\n",
    "    ix = Variable(alpha * x + (1 - alpha) * x_) #Random Weighted Average\n",
    "    ix.requires_grad_(True)\n",
    "    v_ix = critic_x(ix)\n",
    "    v_ix.mean().backward()\n",
    "    gradients = ix.grad\n",
    "    #Gradient Penalty Loss\n",
    "    gp_loss = torch.sqrt(torch.sum(torch.square(gradients).view(-1)))\n",
    "\n",
    "    #Critic has to maximize Cx(Valid X) - Cx(Fake X).\n",
    "    #Maximizing the above is same as minimizing the negative.\n",
    "    wl = critic_score_fake_x - critic_score_valid_x\n",
    "    loss = wl + gp_loss\n",
    "    loss.backward()\n",
    "    optim_cx.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def critic_z_iteration(sample):\n",
    "    optim_cz.zero_grad()\n",
    "\n",
    "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
    "    z = encoder(x)\n",
    "    valid_z = critic_z(z)\n",
    "    valid_z = torch.squeeze(valid_z)\n",
    "    critic_score_valid_z = torch.mean(torch.ones(valid_z.shape) * valid_z)\n",
    "\n",
    "    z_ = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
    "    fake_z = critic_z(z_)\n",
    "    fake_z = torch.squeeze(fake_z)\n",
    "    critic_score_fake_z = torch.mean(torch.ones(fake_z.shape) * fake_z) #Wasserstein Loss\n",
    "\n",
    "    wl = critic_score_fake_z - critic_score_valid_z\n",
    "\n",
    "    alpha = torch.rand(z.shape)\n",
    "    iz = Variable(alpha * z + (1 - alpha) * z_) #Random Weighted Average\n",
    "    iz.requires_grad_(True)\n",
    "    v_iz = critic_z(iz)\n",
    "    v_iz.mean().backward()\n",
    "    gradients = iz.grad\n",
    "    gp_loss = torch.sqrt(torch.sum(torch.square(gradients).view(-1)))\n",
    "\n",
    "    loss = wl + gp_loss\n",
    "    loss.backward()\n",
    "    optim_cz.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def encoder_iteration(sample):\n",
    "    optim_enc.zero_grad()\n",
    "\n",
    "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
    "    valid_x = critic_x(x)\n",
    "    valid_x = torch.squeeze(valid_x)\n",
    "    critic_score_valid_x = torch.mean(torch.ones(valid_x.shape) * valid_x) #Wasserstein Loss\n",
    "\n",
    "    z = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
    "    x_ = decoder(z)\n",
    "    fake_x = critic_x(x_)\n",
    "    fake_x = torch.squeeze(fake_x)\n",
    "    critic_score_fake_x = torch.mean(torch.ones(fake_x.shape) * fake_x)\n",
    "\n",
    "    enc_z = encoder(x)\n",
    "    gen_x = decoder(enc_z)\n",
    "\n",
    "    mse = mse_loss(x.float(), gen_x.float())\n",
    "    loss_enc = mse + critic_score_valid_x - critic_score_fake_x\n",
    "    loss_enc.backward(retain_graph=True)\n",
    "    optim_enc.step()\n",
    "\n",
    "    return loss_enc\n",
    "\n",
    "def decoder_iteration(sample):\n",
    "    optim_dec.zero_grad()\n",
    "\n",
    "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
    "    z = encoder(x)\n",
    "    valid_z = critic_z(z)\n",
    "    valid_z = torch.squeeze(valid_z)\n",
    "    critic_score_valid_z = torch.mean(torch.ones(valid_z.shape) * valid_z)\n",
    "\n",
    "    z_ = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
    "    fake_z = critic_z(z_)\n",
    "    fake_z = torch.squeeze(fake_z)\n",
    "    critic_score_fake_z = torch.mean(torch.ones(fake_z.shape) * fake_z)\n",
    "\n",
    "    enc_z = encoder(x)\n",
    "    gen_x = decoder(enc_z)\n",
    "\n",
    "    mse = mse_loss(x.float(), gen_x.float())\n",
    "    loss_dec = mse + critic_score_valid_z - critic_score_fake_z\n",
    "    loss_dec.backward(retain_graph=True)\n",
    "    optim_dec.step()\n",
    "\n",
    "    return loss_dec\n",
    "\n",
    "\n",
    "def train(n_epochs=2000):\n",
    "    logging.debug('Starting training')\n",
    "    cx_epoch_loss = list()\n",
    "    cz_epoch_loss = list()\n",
    "    encoder_epoch_loss = list()\n",
    "    decoder_epoch_loss = list()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        logging.debug('Epoch {}'.format(epoch))\n",
    "        n_critics = 5\n",
    "\n",
    "        cx_nc_loss = list()\n",
    "        cz_nc_loss = list()\n",
    "\n",
    "        for i in range(n_critics):\n",
    "            cx_loss = list()\n",
    "            cz_loss = list()\n",
    "\n",
    "            for batch, sample in enumerate(train_loader):\n",
    "                loss = critic_x_iteration(sample)\n",
    "                cx_loss.append(loss)\n",
    "\n",
    "                loss = critic_z_iteration(sample)\n",
    "                cz_loss.append(loss)\n",
    "\n",
    "            cx_nc_loss.append(torch.mean(torch.tensor(cx_loss)))\n",
    "            cz_nc_loss.append(torch.mean(torch.tensor(cz_loss)))\n",
    "\n",
    "        logging.debug('Critic training done in epoch {}'.format(epoch))\n",
    "        encoder_loss = list()\n",
    "        decoder_loss = list()\n",
    "\n",
    "        for batch, sample in enumerate(train_loader):\n",
    "            enc_loss = encoder_iteration(sample)\n",
    "            dec_loss = decoder_iteration(sample)\n",
    "            encoder_loss.append(enc_loss)\n",
    "            decoder_loss.append(dec_loss)\n",
    "\n",
    "        cx_epoch_loss.append(torch.mean(torch.tensor(cx_nc_loss)))\n",
    "        cz_epoch_loss.append(torch.mean(torch.tensor(cz_nc_loss)))\n",
    "        encoder_epoch_loss.append(torch.mean(torch.tensor(encoder_loss)))\n",
    "        decoder_epoch_loss.append(torch.mean(torch.tensor(decoder_loss)))\n",
    "        logging.debug('Encoder decoder training done in epoch {}'.format(epoch))\n",
    "        logging.debug('critic x loss {:.3f} critic z loss {:.3f} \\nencoder loss {:.3f} decoder loss {:.3f}\\n'.format(cx_epoch_loss[-1], cz_epoch_loss[-1], encoder_epoch_loss[-1], decoder_epoch_loss[-1]))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(encoder.state_dict(), encoder.encoder_path)\n",
    "            torch.save(decoder.state_dict(), decoder.decoder_path)\n",
    "            torch.save(critic_x.state_dict(), critic_x.critic_x_path)\n",
    "            torch.save(critic_z.state_dict(), critic_z.critic_z_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = pd.read_csv('exchange-2_cpc_results.csv')\n",
    "    #Splitting intro train and test\n",
    "    #TODO could be done in a more pythonic way\n",
    "    train_len = int(0.7 * dataset.shape[0])\n",
    "    dataset[0:train_len].to_csv('train_dataset.csv', index=False)\n",
    "    dataset[train_len:].to_csv('test_dataset.csv', index=False)\n",
    "\n",
    "    train_dataset = SignalDataset(path='train_dataset.csv')\n",
    "    test_dataset = SignalDataset(path='test_dataset.csv')\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    logging.info('Number of train datapoints is {}'.format(len(train_dataset)))\n",
    "    logging.info('Number of samples in train dataset {}'.format(len(train_dataset)))\n",
    "\n",
    "    lr = 1e-6\n",
    "\n",
    "    signal_shape = 100\n",
    "    latent_space_dim = 20\n",
    "    encoder_path = 'models/encoder.pt'\n",
    "    decoder_path = 'models/decoder.pt'\n",
    "    critic_x_path = 'models/critic_x.pt'\n",
    "    critic_z_path = 'models/critic_z.pt'\n",
    "    \n",
    "    encoder = model.Encoder(encoder_path, signal_shape)\n",
    "    decoder = model.Decoder(decoder_path, signal_shape)\n",
    "    critic_x = model.CriticX(critic_x_path, signal_shape)\n",
    "    critic_z = model.CriticZ(critic_z_path)\n",
    "\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "    optim_enc = optim.Adam(encoder.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_dec = optim.Adam(decoder.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_cx = optim.Adam(critic_x.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_cz = optim.Adam(critic_z.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    train(n_epochs=1)\n",
    "\n",
    "    anomaly_detection.test(test_loader, encoder, decoder, critic_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes an array of anomaly scores based on a combination of reconstruction error and critic output\n",
    "params = {\"rec_error_type\": \"dtw\"}\n",
    "\n",
    "primitive = load_primitive(\"orion.primitives.tadgan.score_anomalies\", \n",
    "                           arguments=params)\n",
    "errors, true_index, true, predictions = primitive.produce(y=y, y_hat=y_hat, critic=critic, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average error value: 2.57\n"
     ]
    }
   ],
   "source": [
    "print(\"average error value: {:.2f}\".format(errors.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2817,), (2718,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors.shape, true_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracts anomalies from sequences of errors following the approach\n",
    "params = {\n",
    "    \"window_size_portion\": 0.33, \n",
    "    \"window_step_size_portion\": 0.1,\n",
    "    \"fixed_threshold\": True\n",
    "}\n",
    "\n",
    "primitive = load_primitive(\"orion.primitives.timeseries_anomalies.find_anomalies\", \n",
    "                           arguments=params)\n",
    "e = primitive.produce(errors=errors, index=true_index)\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion.evaluation.contextual import contextual_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = load_anomalies('S-1')\n",
    "data_span = (1222819200, 1442016000)\n",
    "anomalies = [(int(i[0]), int(i[1])) for i in e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lcwong/opt/anaconda3/envs/orion-env/lib/python3.7/site-packages/orion/evaluation/common.py:70: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * (precision * recall) / (precision + recall)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = data_span\n",
    "contextual_f1_score(ground_truth, anomalies, start=start, end=end, weighted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_5/f7yzqnxs6694l3c_yk9mjpzr0000gn/T/ipykernel_89833/397572109.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiHeadAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def positional_encoding_map(max_len, dim, base=10000):\n",
    "    pe = [[pos / np.power(base, (i - i % 2) / dim) for i in range(dim)]\n",
    "          for pos in range(max_len)]\n",
    "    # shape -> [max_len, dim]\n",
    "    pe = np.array(pe)\n",
    "    pe[:, ::2] = np.sin(pe[:, ::2])\n",
    "    pe[:, 1::2] = np.cos(pe[:, 1::2])\n",
    "\n",
    "    return pe\n",
    "\n",
    "\n",
    "def label_smoothing(input_onehot, epsilon=0.1):\n",
    "    depth = input_onehot.get_shape().as_list()[-1]\n",
    "    return (1 - epsilon) * input_onehot + epsilon / depth\n",
    "\n",
    "\n",
    "def layer_norm(inputs, epsilon=1e-8, scope='layer_norm'):\n",
    "    \"\"\"\n",
    "    :param inputs: a tensor.\n",
    "    :param epsilon: a float number for preventing zero division.\n",
    "    :param scope:\n",
    "    :return: a tensor with the same shape and data type as 'inputs'.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        param_shape = inputs.get_shape()[-1:]\n",
    "        gamma = tf.get_variable('gamma', param_shape, initializer=tf.ones_initializer())\n",
    "        beta = tf.get_variable('beta', param_shape, initializer=tf.zeros_initializer())\n",
    "\n",
    "        mean, variance = tf.nn.moments(inputs, axes=[-1], keep_dims=True)\n",
    "        inputs_norm = (inputs - mean) / (variance + epsilon ** 0.5)\n",
    "        output = gamma * inputs_norm + beta\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def multi_head_attention(queries, keys, values, n_heads, key_mask, causality, scope):\n",
    "    \"\"\" Split the input into n_heads heads, then calculate the context vector for each head, and merge all\n",
    "    context vectors into output.\n",
    "    :param queries: the query sequences. [..., n_queries, hidden_dim]\n",
    "    :param keys: the key sequences. [..., n_keys, hidden_dim]\n",
    "    :param values: the value sequences whose length is same as keys. [..., n_keys, hidden_dim]\n",
    "    :param n_heads: the number of heads\n",
    "    :param key_mask: mask for keys. [..., n_keys]\n",
    "    :param causality: mask for queries. True or False\n",
    "    :param scope: the variable scope name\n",
    "    :return: context vector. [..., n_queries, hidden_dim]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        hidden_dim = queries.get_shape().as_list()[-1]\n",
    "        # transform input\n",
    "        queries = layers.Dense(hidden_dim, name='Q_dense')(queries)\n",
    "        keys = layers.Dense(hidden_dim, name='K_dense')(keys)\n",
    "        values = layers.Dense(hidden_dim, name='V_dense')(values)\n",
    "\n",
    "        # split the whole input into the part input for each head\n",
    "        # [n_heads, ..., n_queries, hidden_dim / n_heads]\n",
    "        queries = tf.stack(tf.split(queries, n_heads, axis=-1), axis=0)\n",
    "        # [n_heads, ..., n_keys, hidden_dim / n_heads]\n",
    "        keys = tf.stack(tf.split(keys, n_heads, axis=-1), axis=0)\n",
    "        # [n_heads, ..., n_keys, hidden_dim / n_heads]\n",
    "        values = tf.stack(tf.split(values, n_heads, axis=-1), axis=0)\n",
    "\n",
    "        # [n_heads, ..., n_queries, hidden_dim / n_heads]p\n",
    "        context_vector = scaled_dot_product_attention(queries, keys, values, key_mask, causality)\n",
    "        # [..., n_queries, hidden_dim]\n",
    "        context_vector = tf.concat(tf.unstack(context_vector, axis=0), axis=-1)\n",
    "\n",
    "        # merge all outputs of each head\n",
    "        output = layers.Dense(hidden_dim, name='head_merge')(context_vector)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(queries, keys, values, key_mask=None, causality=False):\n",
    "    \"\"\" Calculate the context vector using scaled dot product attention mechanism.\n",
    "    :param queries: [..., n_queries, hidden_dim]\n",
    "    :param keys: [..., n_keys, hidden_dim]\n",
    "    :param values: [..., n_keys, hidden_dim]\n",
    "    :param key_mask: mask for keys. the flag of the point to be masked is 0, and the other is 1. [..., n_keys]\n",
    "    :param causality: mask for queries. True or False.\n",
    "    :return: context vector. [..., n_queries, hidden_dim]\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope('scaled_attention'):\n",
    "        # general setting\n",
    "        MASKED_VAL = -2 ** 31\n",
    "        score_fn = lambda q, k: tf.matmul(queries, keys, transpose_b=True) / tf.sqrt(\n",
    "            tf.cast(q.get_shape().as_list()[-1], dtype=tf.float32))\n",
    "\n",
    "        score = score_fn(queries, keys)     # [..., n_queries, n_keys]\n",
    "\n",
    "        # mask score by mask of keys\n",
    "        if key_mask is not None:\n",
    "            key_mask_mat = (1.0 - key_mask) * MASKED_VAL  # [..., n_keys]\n",
    "            key_mask_mat = tf.expand_dims(key_mask_mat, -2)     # [..., 1, n_keys]\n",
    "            score += key_mask_mat\n",
    "\n",
    "        # mask score by causality of queries\n",
    "        # mask values for the upper right area, including the diagonal\n",
    "        if causality:\n",
    "\n",
    "            ones_mat = tf.ones_like(score)\n",
    "            zeros_mat = tf.zeros_like(score)\n",
    "            masked_val_mat = ones_mat * MASKED_VAL\n",
    "\n",
    "            # [..., n_queries, n_keys]\n",
    "            lower_diag_masks = tf.linalg.LinearOperatorLowerTriangular(ones_mat).to_dense()\n",
    "\n",
    "            score = tf.where(tf.equal(lower_diag_masks, 0),\n",
    "                             masked_val_mat,\n",
    "                             score)\n",
    "            # [..., n_queries, n_keys]\n",
    "            attention_weight = tf.nn.softmax(score, axis=-1)\n",
    "            # attention_weight = tf.where(tf.equal(lower_diag_masks, 0),\n",
    "            #                             zeros_mat,\n",
    "            #                             attention_weight)\n",
    "\n",
    "        else:\n",
    "            # [..., n_queries, n_keys]\n",
    "            attention_weight = tf.nn.softmax(score, axis=2)\n",
    "\n",
    "        # [..., n_queries, hidden_dim]\n",
    "        context_vector = tf.matmul(attention_weight, values)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "def ffn(x, dims, activation=tf.nn.relu, scope='ffn'):\n",
    "    \"\"\" Feed Forward Network.\n",
    "    :param x:\n",
    "    :param dims: a list of each layer dimension.\n",
    "    :param activation: activation function for inner layer.\n",
    "    :param scope:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        for dim in dims[:-1]:\n",
    "            x = layers.Dense(dim, activation=activation)(x)\n",
    "\n",
    "        output = layers.Dense(dims[-1])(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
