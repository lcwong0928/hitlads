{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "\n",
    "from src.configuration.constants import PROCESSED_DATA_DIRECTORY, ROOT_DIRECTORY, INTERIM_DATA_DIRECTORY\n",
    "from src.utils.dataset import load_dataset\n",
    "\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from numpy import expand_dims, zeros, ones, asarray\n",
    "from numpy.random import randn, randint\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "from mlprimitives import load_primitive\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "from orion.evaluation.contextual import contextual_f1_score\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "from mlprimitives import load_primitive\n",
    "\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/semi-supervised-generative-adversarial-network/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 0: Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('interim', 'SMAP')\n",
    "\n",
    "train_split = dataset['train'][dataset['train'].signal == 'A3Benchmark-TS12']\n",
    "test_split = dataset['test'][dataset['test'].signal == 'A3Benchmark-TS12']\n",
    "anomalies_split = dataset['anomaly'][dataset['anomaly'].signal == 'A3Benchmark-TS12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_split.anomaly.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train = train_split['index'].astype(int)\n",
    "X_train = train_split[['anomaly'] + list(train_split.columns)[5:]]\n",
    "\n",
    "index_test = test_split['index'].astype(int).reset_index(drop=True)\n",
    "X_test = test_split[['anomaly'] + list(test_split.columns)[5:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_processing(X, index):\n",
    "    primitives = []\n",
    "    \n",
    "    \n",
    "    # This primitive is an imputation transformer for filling missing values\n",
    "    params = {\n",
    "        'X': X\n",
    "    }\n",
    "    primitive = load_primitive('sklearn.impute.SimpleImputer', arguments=params)\n",
    "    primitive.fit()\n",
    "    primitives.append(primitive)\n",
    "    X = primitive.produce(X=X)\n",
    "    print(primitive, X.shape)\n",
    "    \n",
    "    # This primitive transforms features by scaling each feature to a given range\n",
    "    params = {\n",
    "        \"feature_range\": [-1, 1], \n",
    "        'X': X,\n",
    "    }\n",
    "    primitive = load_primitive('sklearn.preprocessing.MinMaxScaler', arguments=params)\n",
    "    primitive.fit()\n",
    "    primitives.append(primitive)\n",
    "    X = primitive.produce(X=X)\n",
    "    print(primitive, X.shape)\n",
    "    \n",
    "    # Uses a rolling window approach to create the sub-sequences out of time series data\n",
    "    params = {\n",
    "        \"target_column\": 0, \n",
    "        \"window_size\": 100, \n",
    "        'target_size': 1, \n",
    "        'step_size': 1\n",
    "    }\n",
    "    primitive = load_primitive('mlprimitives.custom.timeseries_preprocessing.rolling_window_sequences',\n",
    "                               arguments=params)\n",
    "    primitives.append(primitive)\n",
    "    X, y, index, target_index = primitive.produce(X=X, index=index)\n",
    "\n",
    "    # Target / target size is the next interval that is trying to predict.\n",
    "    # Index is the start of the interval\n",
    "    print(primitive, X.shape, y.shape, index.shape, target_index.shape)\n",
    "    \n",
    "    return X, index, primitives\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLBlock - sklearn.impute.SimpleImputer (1120, 8)\n",
      "MLBlock - sklearn.preprocessing.MinMaxScaler (1120, 8)\n",
      "MLBlock - mlprimitives.custom.timeseries_preprocessing.rolling_window_sequences (1020, 100, 8) (1020, 1) (1020,) (1020,)\n"
     ]
    }
   ],
   "source": [
    "X, index, primitives = fit_processing(X_train, index_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1020, 100, 7), (1020, 100, 1), (1020,), (1020,))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:, :, 1:]\n",
    "y_train = np.expand_dims(X_train[:, :, 0], 2)\n",
    "index_train = index\n",
    "labels_train = np.array([1 if sum(i) > 0 else 0 for i in X[:, :, 0]])\n",
    "\n",
    "X_train.shape, y_train.shape, index_train.shape, labels_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_processing(X, index, primitives):\n",
    "    X = primitives[0].produce(X=X)\n",
    "    X = primitives[1].produce(X=X)\n",
    "    X, y, index, target_index = primitives[2].produce(X=X, index=index)\n",
    "    return X, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, index = produce_processing(X_test, index_test, primitives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((460, 100, 7), (460, 100, 1), (460,), (460,))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X[:, :, 1:]\n",
    "y_test = np.expand_dims(X_test[:, :, 0], 2)\n",
    "index_test = index\n",
    "labels_test = np.array([1 if sum(i) > 0 else 0 for i in X[:, :, 0]])\n",
    "\n",
    "\n",
    "X_test.shape, y_test.shape, index_test.shape, labels_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Semi-supervised GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Discriminator Models With Shared Weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(input_shape, lstm_units, dense_units, encoder_reshape_shape):\n",
    "    x = Input(shape=input_shape)\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=lstm_units, return_sequences=True)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=dense_units))\n",
    "    model.add(Reshape(target_shape=encoder_reshape_shape))\n",
    "    return Model(x, model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(input_shape, generator_reshape_dim, generator_reshape_shape):\n",
    "    x = Input(shape=input_shape)\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=generator_reshape_dim))\n",
    "    model.add(Reshape(target_shape=generator_reshape_shape))\n",
    "    model.add(Bidirectional(LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode='concat'))\n",
    "    model.add(UpSampling1D(size=2))\n",
    "    model.add(Bidirectional(LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode='concat'))\n",
    "    model.add(TimeDistributed(Dense(units=1)))\n",
    "    model.add(Activation(activation='tanh'))\n",
    "    return Model(x, model(x))\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_critic_x(input_shape, n_classes):\n",
    "    x = Input(shape=input_shape)\n",
    "    model = Sequential()\n",
    "    \n",
    "    for _ in range(4):\n",
    "        model.add(Conv1D(filters=64, kernel_size=5))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(rate=0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Unsupervised model\n",
    "    unsup_out = Dense(1, activation='sigmoid')(model(x))\n",
    "    d_unsup_model = Model(x, unsup_out)\n",
    "    \n",
    "    # Supervised model\n",
    "    sup_out = Dense(n_classes, activation='softmax')(model(x))\n",
    "    d_sup_model = Model(x, sup_out)\n",
    "    \n",
    "    return d_unsup_model, d_sup_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_critic_z(input_shape, dense_units=20):\n",
    "    x = Input(shape=input_shape)\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for _ in range(2):\n",
    "        model.add(Dense(units=dense_units))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(rate=0.2))\n",
    "        \n",
    "    model.add(Dense(units=1))\n",
    "    return Model(x, model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=X_test[0].shape\n",
    "target_shape=y_test[0].shape\n",
    "latent_dim=20\n",
    "learning_rate=0.0005\n",
    "epochs=20\n",
    "batch_size=64\n",
    "iterations_critic=5\n",
    "latent_shape = (latent_dim, 1)\n",
    "n_classes = 2\n",
    "\n",
    "shape = np.asarray(X_test)[0].shape\n",
    "length = shape[0]\n",
    "target_shape = np.asarray(y_test)[0].shape\n",
    "\n",
    "\n",
    "generator_reshape_dim = length // 2\n",
    "generator_reshape_shape = (length // 2, 1)\n",
    "encoder_reshape_shape = latent_shape\n",
    "\n",
    "encoder_input_shape = shape\n",
    "generator_input_shape = latent_shape\n",
    "critic_x_input_shape = target_shape\n",
    "critic_z_input_shape = latent_shape\n",
    "\n",
    "lstm_units = 100\n",
    "dense_units = 20\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = build_encoder(\n",
    "    input_shape=encoder_input_shape,\n",
    "    lstm_units=lstm_units,\n",
    "    dense_units=dense_units,\n",
    "    encoder_reshape_shape=encoder_reshape_shape,\n",
    ")\n",
    "\n",
    "generator = build_generator(\n",
    "    input_shape=generator_input_shape,\n",
    "    generator_reshape_dim=generator_reshape_dim,\n",
    "    generator_reshape_shape=generator_reshape_shape,\n",
    ")\n",
    "\n",
    "critic_x_unsup, critic_x_sup = build_critic_x(\n",
    "    input_shape=critic_x_input_shape,\n",
    "    n_classes=n_classes\n",
    ")\n",
    "\n",
    "critic_z = build_critic_z(\n",
    "    input_shape=critic_z_input_shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "def _gradient_penalty_loss(y_true, y_pred, averaged_samples):\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr, axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "    return K.mean(gradient_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.trainable = False\n",
    "encoder.trainable = False\n",
    "\n",
    "x = Input(shape=input_shape)\n",
    "y = Input(shape=target_shape)\n",
    "z = Input(shape=(latent_dim, 1))\n",
    "\n",
    "x_ = generator(z)\n",
    "z_ = encoder(x)\n",
    "fake_x = critic_x_unsup(x_) # Fake\n",
    "valid_x = critic_x_unsup(y) # Truth\n",
    "\n",
    "label = critic_x_sup(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic X Unsupervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           [(None, 20, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           [(None, 100, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_9 (Model)                 (None, 100, 1)       133787      input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_4 (TensorFlowOp [(64, 100, 1)]       0           input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_5 (TensorFlowOp [(64, 100, 1)]       0           model_9[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_2 (TensorFlowOp [(64, 100, 1)]       0           tf_op_layer_mul_4[0][0]          \n",
      "                                                                 tf_op_layer_mul_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "model_10 (Model)                multiple             67393       model_9[1][0]                    \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 tf_op_layer_add_2[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 201,180\n",
      "Trainable params: 67,393\n",
      "Non-trainable params: 133,787\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "alpha = K.random_uniform((64, 1, 1))\n",
    "interpolated_x = (alpha * [y, x_][0]) + ((1 - alpha) * [y, x_][1])\n",
    "validity_interpolated_x = critic_x_unsup(interpolated_x)\n",
    "partial_gp_loss_x = partial(_gradient_penalty_loss, averaged_samples=interpolated_x)\n",
    "partial_gp_loss_x.__name__ = 'gradient_penalty'\n",
    "critic_x_unsup_model = Model(inputs=[y, z], outputs=[valid_x, fake_x,validity_interpolated_x])\n",
    "critic_x_unsup_model.compile(loss=[_wasserstein_loss, _wasserstein_loss, partial_gp_loss_x], \n",
    "                       optimizer=optimizer, loss_weights=[1, 1, 10])\n",
    "critic_x_unsup_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic X Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 100, 1)]          0         \n",
      "_________________________________________________________________\n",
      "sequential_6 (Sequential)    multiple                  62016     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 10754     \n",
      "=================================================================\n",
      "Total params: 72,770\n",
      "Trainable params: 72,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic_x_sup.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=optimizer, \n",
    "    metrics=['accuracy'], # f1, precision, recall\n",
    ")\n",
    "critic_x_sup_model = critic_x_sup\n",
    "critic_x_sup_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic Z Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           [(None, 100, 7)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 20, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_8 (Model)                 (None, 20, 1)        486420      input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_6 (TensorFlowOp [(64, 20, 1)]        0           input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_7 (TensorFlowOp [(64, 20, 1)]        0           model_8[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_3 (TensorFlowOp [(64, 20, 1)]        0           tf_op_layer_mul_6[0][0]          \n",
      "                                                                 tf_op_layer_mul_7[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "model_12 (Model)                multiple             861         model_8[1][0]                    \n",
      "                                                                 input_16[0][0]                   \n",
      "                                                                 tf_op_layer_add_3[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 487,281\n",
      "Trainable params: 861\n",
      "Non-trainable params: 486,420\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fake_z = critic_z(z_)\n",
    "valid_z = critic_z(z)\n",
    "alpha = K.random_uniform((64, 1, 1))\n",
    "interpolated_z = (alpha * [z, z_][0]) + ((1 - alpha) * [z, z_][1])\n",
    "validity_interpolated_z = critic_z(interpolated_z)\n",
    "partial_gp_loss_z = partial(_gradient_penalty_loss, averaged_samples=interpolated_z)\n",
    "partial_gp_loss_z.__name__ = 'gradient_penalty'\n",
    "critic_z_model = tf.keras.Model(inputs=[x, z], outputs=[valid_z, fake_z,validity_interpolated_z])\n",
    "critic_z_model.compile(loss=[_wasserstein_loss, _wasserstein_loss,\n",
    "                                  partial_gp_loss_z], optimizer=optimizer,\n",
    "                            loss_weights=[1, 1, 10])\n",
    "critic_z_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           [(None, 20, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           [(None, 100, 7)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_9 (Model)                 (None, 100, 1)       133787      input_17[0][0]                   \n",
      "                                                                 model_8[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_8 (Model)                 (None, 20, 1)        486420      input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_10 (Model)                multiple             67393       model_9[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_12 (Model)                multiple             861         model_8[2][0]                    \n",
      "==================================================================================================\n",
      "Total params: 688,461\n",
      "Trainable params: 620,207\n",
      "Non-trainable params: 68,254\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic_x_sup.trainable = False\n",
    "critic_x_unsup.trainable = False\n",
    "critic_z.trainable = False\n",
    "generator.trainable = True\n",
    "encoder.trainable = True\n",
    "\n",
    "z_gen = Input(shape=(latent_dim, 1))\n",
    "x_gen_ = generator(z_gen)\n",
    "x_gen = Input(shape=input_shape)\n",
    "z_gen_ = encoder(x_gen)\n",
    "x_gen_rec = generator(z_gen_)\n",
    "fake_gen_x = critic_x_unsup(x_gen_)\n",
    "fake_gen_z = critic_z(z_gen_)\n",
    "\n",
    "encoder_generator_model = Model([x_gen, z_gen], [fake_gen_x, fake_gen_z, x_gen_rec])\n",
    "encoder_generator_model.compile(loss=[_wasserstein_loss, _wasserstein_loss,'mse'], \n",
    "                                optimizer=optimizer,\n",
    "                                loss_weights=[1, 1, 10])\n",
    "\n",
    "encoder_generator_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_supervised_samples(y, labels, limit=None):\n",
    "    labels = pd.Series(labels)\n",
    "    \n",
    "    sup_anomaly_index = labels[labels==1].index\n",
    "    if limit is not None:\n",
    "        sup_anomaly_index = random.choices(sup_anomaly_index, k=limit)\n",
    "        \n",
    "    sup_anomaly_samples = y[sup_anomaly_index]\n",
    "    num_anomaly_samples = len(sup_anomaly_samples)\n",
    "    \n",
    "    sup_non_anomaly_index = random.choices(labels[labels==0].index,  k=num_anomaly_samples)\n",
    "    sup_non_anomaly_samples = y[sup_non_anomaly_index]\n",
    "    \n",
    "    sup_samples = np.concatenate((sup_anomaly_samples, sup_non_anomaly_samples))\n",
    "    sup_labels = np.concatenate((np.ones(len(sup_anomaly_samples)), np.zeros(len(sup_non_anomaly_samples))))\n",
    "    \n",
    "    unsup_samples = y[~np.isin(np.arange(len(y)), list(sup_anomaly_index) + list(sup_non_anomaly_index))]\n",
    "    \n",
    "    return sup_samples, sup_labels, unsup_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 100, 1), (0,), (460, 100, 1))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sup_samples, sup_labels, unsup_samples = split_supervised_samples(y_test, labels_test, 0)\n",
    "\n",
    "sup_samples.shape, sup_labels.shape, unsup_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_supervised_samples(n_samples, X, labels, n_classes):\n",
    "    \"\"\"Select a supervised subset of the dataset, ensures classes are balanced.\"\"\"\n",
    "    X_list, y_list = list(), list()\n",
    "    n_per_class = int(n_samples / n_classes)\n",
    "    for i in range(n_classes):\n",
    "        X_with_class = X[labels == i]\n",
    "        ix = randint(0, len(X_with_class), n_per_class)\n",
    "        [X_list.append(X_with_class[j]) for j in ix]\n",
    "        [y_list.append(i) for j in ix]\n",
    "    return asarray(X_list), asarray(y_list)\n",
    "\n",
    "def generate_real_samples(n_samples, X, labels):\n",
    "    \"\"\"Randomly select real samples.\"\"\"\n",
    "    ix = randint(0, X.shape[0], n_samples)\n",
    "    X, labels = X[ix], labels[ix]\n",
    "    y = ones((n_samples, 1))\n",
    "    return [X, labels], y\n",
    "\n",
    "def generate_latent_points(n_samples, latent_dim):\n",
    "    \"\"\"Generate points in latent space as input for the generator.\"\"\"\n",
    "    return np.random.normal(size=(n_samples, latent_dim, 1))\n",
    " \n",
    "def generate_fake_samples(n_samples, latent_dim, generator):\n",
    "    \"\"\"Generate n fake examples with class labels.\"\"\"\n",
    "    z = generate_latent_points(n_samples, latent_dim)\n",
    "    x_ = generator(z)\n",
    "    y = zeros((n_samples, 1))\n",
    "    return x_, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_5/f7yzqnxs6694l3c_yk9mjpzr0000gn/T/ipykernel_34313/2248222336.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msup_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msup_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsup_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_supervised_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/_5/f7yzqnxs6694l3c_yk9mjpzr0000gn/T/ipykernel_34313/1555286971.py\u001b[0m in \u001b[0;36msplit_supervised_samples\u001b[0;34m(y, labels, limit)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msup_anomaly_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msup_anomaly_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msup_anomaly_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msup_anomaly_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msup_anomaly_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hitlads-env/lib/python3.7/random.py\u001b[0m in \u001b[0;36mchoices\u001b[0;34m(self, population, weights, cum_weights, k)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0m_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0mcum_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_itertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hitlads-env/lib/python3.7/random.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0m_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0mcum_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_itertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hitlads-env/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3957\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3958\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "enable_supervised = None\n",
    "\n",
    "fake = np.ones((batch_size, 1))\n",
    "valid = -np.ones((batch_size, 1))\n",
    "delta = np.ones((batch_size, 1))\n",
    "\n",
    "sup_samples, sup_labels, unsup_samples = split_supervised_samples(y_train, labels_train, 1)\n",
    "\n",
    "\n",
    "indices = np.arange(X_train.shape[0])\n",
    "for epoch in range(1, epochs + 1):\n",
    "    np.random.shuffle(indices)\n",
    "    X_ = X_train[indices]\n",
    "    y_ = y_train[indices]\n",
    "\n",
    "    epoch_g_loss = []\n",
    "    epoch_cx_loss = []\n",
    "    epoch_cz_loss = []\n",
    "    epoch_cx_sup_loss = []\n",
    "\n",
    "    minibatches_size = batch_size * iterations_critic\n",
    "    num_minibatches = int(X_.shape[0] // minibatches_size)\n",
    "\n",
    "    for i in range(num_minibatches):\n",
    "        minibatch = X_[i * minibatches_size: (i + 1) * minibatches_size]\n",
    "        y_minibatch = y_[i * minibatches_size: (i + 1) * minibatches_size]\n",
    "        \n",
    "        if enable_supervised:\n",
    "            [X_sup_real, y_sup_real], _ = generate_real_samples(minibatches_size, sup_samples, sup_labels)\n",
    "            epoch_cx_sup_loss.append(\n",
    "                critic_x_sup_model.train_on_batch(X_sup_real, y_sup_real)\n",
    "            )\n",
    "\n",
    "        for j in range(iterations_critic):\n",
    "            x = minibatch[j * batch_size: (j + 1) * batch_size]\n",
    "            y = y_minibatch[j * batch_size: (j + 1) * batch_size]\n",
    "            z = np.random.normal(size=(batch_size, latent_dim, 1))\n",
    "            \n",
    "            epoch_cx_loss.append(\n",
    "                critic_x_unsup_model.train_on_batch([y, z], [valid, fake, delta]))\n",
    "            epoch_cz_loss.append(\n",
    "                critic_z_model.train_on_batch([x, z], [valid, fake, delta]))\n",
    "\n",
    "        epoch_g_loss.append(\n",
    "            encoder_generator_model.train_on_batch([x, z], [valid, valid, y]))\n",
    "\n",
    "    cx_loss = np.mean(np.array(epoch_cx_loss), axis=0)\n",
    "    cz_loss = np.mean(np.array(epoch_cz_loss), axis=0)\n",
    "    g_loss = np.mean(np.array(epoch_g_loss), axis=0)\n",
    "    cx_sup_loss = np.mean(np.array(epoch_cx_sup_loss), axis=0)\n",
    "    \n",
    "    print('Epoch: {}/{}, [Dx loss: {}] [Dx_sup loss: {}] [Dz loss: {}] [G loss: {}]'.format(\n",
    "        epoch, epochs, cx_loss, cx_sup_loss, cz_loss, g_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of unsupervised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((460, 20, 1), (460, 100, 1), (460, 1))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_ = encoder.predict(X_test)\n",
    "y_hat = generator.predict(z_)\n",
    "critic = critic_x_unsup.predict(y_test)\n",
    "\n",
    "z_.shape, y_hat.shape, critic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((559,), (1020,))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computes an array of anomaly scores based on a combination of reconstruction error and critic output\n",
    "params = {\"rec_error_type\": \"dtw\"}\n",
    "\n",
    "primitive = load_primitive(\"orion.primitives.tadgan.score_anomalies\", arguments=params)\n",
    "errors, true_index, true, predictions = primitive.produce(y=y_test, y_hat=y_hat, critic=critic, index=index_train)\n",
    "\n",
    "errors.shape, true_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracts anomalies from sequences of errors following the approach\n",
    "params = {\n",
    "    \"window_size_portion\": 0.1, \n",
    "    \"window_step_size_portion\": 0.1,\n",
    "    \"fixed_threshold\": True\n",
    "}\n",
    "\n",
    "primitive = load_primitive(\"orion.primitives.timeseries_anomalies.find_anomalies\", \n",
    "                           arguments=params)\n",
    "e = primitive.produce(errors=errors, index=true_index)\n",
    "\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>name</th>\n",
       "      <th>signal</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yahoo</td>\n",
       "      <td>A3</td>\n",
       "      <td>A3Benchmark-TS12</td>\n",
       "      <td>1.416766e+09</td>\n",
       "      <td>1.416766e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yahoo</td>\n",
       "      <td>A3</td>\n",
       "      <td>A3Benchmark-TS12</td>\n",
       "      <td>1.418414e+09</td>\n",
       "      <td>1.418414e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yahoo</td>\n",
       "      <td>A3</td>\n",
       "      <td>A3Benchmark-TS12</td>\n",
       "      <td>1.418627e+09</td>\n",
       "      <td>1.418627e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yahoo</td>\n",
       "      <td>A3</td>\n",
       "      <td>A3Benchmark-TS12</td>\n",
       "      <td>1.418670e+09</td>\n",
       "      <td>1.418670e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yahoo</td>\n",
       "      <td>A3</td>\n",
       "      <td>A3Benchmark-TS12</td>\n",
       "      <td>1.419592e+09</td>\n",
       "      <td>1.419592e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Yahoo</td>\n",
       "      <td>A3</td>\n",
       "      <td>A3Benchmark-TS12</td>\n",
       "      <td>1.420009e+09</td>\n",
       "      <td>1.420009e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yahoo</td>\n",
       "      <td>A3</td>\n",
       "      <td>A3Benchmark-TS12</td>\n",
       "      <td>1.420200e+09</td>\n",
       "      <td>1.420200e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Yahoo</td>\n",
       "      <td>A3</td>\n",
       "      <td>A3Benchmark-TS12</td>\n",
       "      <td>1.420571e+09</td>\n",
       "      <td>1.420571e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Yahoo</td>\n",
       "      <td>A3</td>\n",
       "      <td>A3Benchmark-TS12</td>\n",
       "      <td>1.420992e+09</td>\n",
       "      <td>1.420992e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Yahoo</td>\n",
       "      <td>A3</td>\n",
       "      <td>A3Benchmark-TS12</td>\n",
       "      <td>1.421039e+09</td>\n",
       "      <td>1.421039e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Yahoo</td>\n",
       "      <td>A3</td>\n",
       "      <td>A3Benchmark-TS12</td>\n",
       "      <td>1.421849e+09</td>\n",
       "      <td>1.421849e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Yahoo</td>\n",
       "      <td>A3</td>\n",
       "      <td>A3Benchmark-TS12</td>\n",
       "      <td>1.422176e+09</td>\n",
       "      <td>1.422176e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Yahoo</td>\n",
       "      <td>A3</td>\n",
       "      <td>A3Benchmark-TS12</td>\n",
       "      <td>1.422360e+09</td>\n",
       "      <td>1.422360e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source name            signal         start           end\n",
       "0   Yahoo   A3  A3Benchmark-TS12  1.416766e+09  1.416766e+09\n",
       "1   Yahoo   A3  A3Benchmark-TS12  1.418414e+09  1.418414e+09\n",
       "2   Yahoo   A3  A3Benchmark-TS12  1.418627e+09  1.418627e+09\n",
       "3   Yahoo   A3  A3Benchmark-TS12  1.418670e+09  1.418670e+09\n",
       "4   Yahoo   A3  A3Benchmark-TS12  1.419592e+09  1.419592e+09\n",
       "5   Yahoo   A3  A3Benchmark-TS12  1.420009e+09  1.420009e+09\n",
       "6   Yahoo   A3  A3Benchmark-TS12  1.420200e+09  1.420200e+09\n",
       "7   Yahoo   A3  A3Benchmark-TS12  1.420571e+09  1.420571e+09\n",
       "8   Yahoo   A3  A3Benchmark-TS12  1.420992e+09  1.420992e+09\n",
       "9   Yahoo   A3  A3Benchmark-TS12  1.421039e+09  1.421039e+09\n",
       "10  Yahoo   A3  A3Benchmark-TS12  1.421849e+09  1.421849e+09\n",
       "11  Yahoo   A3  A3Benchmark-TS12  1.422176e+09  1.422176e+09\n",
       "12  Yahoo   A3  A3Benchmark-TS12  1.422360e+09  1.422360e+09"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1110246980790383e-05"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = anomalies_split\n",
    "anomalies = [(int(i[0]), int(i[1])) for i in e]\n",
    "start, end = index_train[0], index_train[-1]\n",
    "contextual_f1_score(ground_truth, anomalies, start=start, end=end, weighted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 0.08085430968726164)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0, 0.07946026986506748\n",
    "\n",
    "100, 0.09651076466221233\n",
    "\n",
    "300, 0.08085430968726164\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Query Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What did the unsupervised GAN say was an anomaly?\n",
    "z_ = encoder.predict(X_train)\n",
    "y_hat = generator.predict(z_)\n",
    "critic = critic_x_unsup.predict(y_train)\n",
    "\n",
    "params = {\"rec_error_type\": \"dtw\"}\n",
    "\n",
    "primitive = load_primitive(\"orion.primitives.tadgan.score_anomalies\", arguments=params)\n",
    "errors, true_index, true, predictions = primitive.produce(y=y_train, y_hat=y_hat, critic=critic, index=index_train)\n",
    "\n",
    "params = {\n",
    "    \"window_size_portion\": 0.05, \n",
    "    \"window_step_size_portion\": 0.1,\n",
    "    \"fixed_threshold\": True\n",
    "}\n",
    "\n",
    "primitive = load_primitive(\"orion.primitives.timeseries_anomalies.find_anomalies\", \n",
    "                           arguments=params)\n",
    "e = primitive.produce(errors=errors, index=true_index)\n",
    "\n",
    "anomalies = [(int(i[0]), int(i[1])) for i in e]\n",
    "pp_samples = []\n",
    "\n",
    "for s, e in anomalies:\n",
    "    for i in range(s, e):\n",
    "        pp_samples.append(X_sup[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_samples = pd.Series(pp_samples)\n",
    "pp_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What did the supervised GAN get wrong?\n",
    "X_sup, labels_sup\n",
    "labels_sup = pd.Series(labels_sup)\n",
    "\n",
    "sup_predictions = critic_x_sup_model.predict(X_sup)\n",
    "\n",
    "idx = np.argmax(sup_predictions, axis=-1)\n",
    "sup_predictions = np.zeros( sup_predictions.shape )\n",
    "sup_predictions[ np.arange(sup_predictions.shape[0]), idx] = 1\n",
    "sup_predictions = [list(i).index(1.0) for i in sup_predictions]\n",
    "sup_predictions = pd.Series(sup_predictions)\n",
    "\n",
    "# False Positives\n",
    "fp = X_sup[(sup_predictions == 1) & (labels_sup == 0)]\n",
    "fn = X_sup[(sup_predictions == 0) & (labels_sup == 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 100, 1), dtype=float64)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How should we explore datasets?\n",
    "\n",
    "exploration = X_train[np.random.choice([i for i in range(len(X_train))], 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 100, 1)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_set = [\n",
    "    # pp_samples,\n",
    "    fp,\n",
    "    fn,\n",
    "    exploration\n",
    "]\n",
    "annotation_set = np.concatenate(annotation_set)\n",
    "annotation_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Annotation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_5/f7yzqnxs6694l3c_yk9mjpzr0000gn/T/ipykernel_15886/1928229540.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# How do we combine labels from different people?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "# How do we combine labels from different people?\n",
    "\n",
    "labels = labels_train(annotation_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
