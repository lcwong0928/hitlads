{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "\n",
    "from src.configuration.constants import PROCESSED_DATA_DIRECTORY, ROOT_DIRECTORY\n",
    "\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from numpy import expand_dims, zeros, ones, asarray\n",
    "from numpy.random import randn, randint\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/semi-supervised-generative-adversarial-network/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2109, 100, 1), (2109, 100, 1), (2109,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(PROCESSED_DATA_DIRECTORY, 'MSL.pickle'), 'rb') as f:\n",
    "    msl_dataset = pickle.load(f)\n",
    "    \n",
    "X_train = msl_dataset['y_train']\n",
    "y_train = msl_dataset['y_train']\n",
    "labels_train = np.array([random.choice([0, 1]) for i in range(2109)])\n",
    "\n",
    "X_train.shape, y_train.shape, labels_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Semi-supervised GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Discriminator Models With Shared Weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(input_shape, lstm_units, dense_units, encoder_reshape_shape):\n",
    "    x = Input(shape=input_shape)\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=lstm_units, return_sequences=True)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=dense_units))\n",
    "    model.add(Reshape(target_shape=encoder_reshape_shape))\n",
    "    return Model(x, model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(input_shape, generator_reshape_dim, generator_reshape_shape):\n",
    "    x = Input(shape=input_shape)\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=generator_reshape_dim))\n",
    "    model.add(Reshape(target_shape=generator_reshape_shape))\n",
    "    model.add(Bidirectional(LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode='concat'))\n",
    "    model.add(UpSampling1D(size=2))\n",
    "    model.add(Bidirectional(LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode='concat'))\n",
    "    model.add(TimeDistributed(Dense(units=1)))\n",
    "    model.add(Activation(activation='tanh'))\n",
    "    return Model(x, model(x))\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_critic_x(input_shape, n_classes):\n",
    "    x = Input(shape=input_shape)\n",
    "    model = Sequential()\n",
    "    \n",
    "    for _ in range(4):\n",
    "        model.add(Conv1D(filters=64, kernel_size=5))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(rate=0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Unsupervised model\n",
    "    unsup_out = Dense(1, activation='sigmoid')(model(x))\n",
    "    d_unsup_model = Model(x, unsup_out)\n",
    "    \n",
    "    # Supervised model\n",
    "    sup_out = Dense(n_classes + 1, activation='softmax')(model(x))\n",
    "    d_sup_model = Model(x, sup_out)\n",
    "    \n",
    "    return d_unsup_model, d_sup_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_critic_z(input_shape, dense_units=20):\n",
    "    x = Input(shape=input_shape)\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for _ in range(2):\n",
    "        model.add(Dense(units=dense_units))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(rate=0.2))\n",
    "        \n",
    "    model.add(Dense(units=1))\n",
    "    return Model(x, model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=X_train[0].shape\n",
    "target_shape=y_train[0].shape\n",
    "latent_dim=20\n",
    "learning_rate=0.0005\n",
    "epochs=70\n",
    "batch_size=64\n",
    "iterations_critic=5\n",
    "latent_shape = (latent_dim, 1)\n",
    "n_classes = 2\n",
    "\n",
    "shape = np.asarray(X_train)[0].shape\n",
    "length = shape[0]\n",
    "target_shape = np.asarray(y_train)[0].shape\n",
    "\n",
    "\n",
    "generator_reshape_dim = length // 2\n",
    "generator_reshape_shape = (length // 2, 1)\n",
    "encoder_reshape_shape = latent_shape\n",
    "\n",
    "encoder_input_shape = shape\n",
    "generator_input_shape = latent_shape\n",
    "critic_x_input_shape = target_shape\n",
    "critic_z_input_shape = latent_shape\n",
    "\n",
    "lstm_units = 100\n",
    "dense_units = 20\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = build_encoder(\n",
    "    input_shape=encoder_input_shape,\n",
    "    lstm_units=lstm_units,\n",
    "    dense_units=dense_units,\n",
    "    encoder_reshape_shape=encoder_reshape_shape,\n",
    ")\n",
    "\n",
    "generator = build_generator(\n",
    "    input_shape=generator_input_shape,\n",
    "    generator_reshape_dim=generator_reshape_dim,\n",
    "    generator_reshape_shape=generator_reshape_shape,\n",
    ")\n",
    "\n",
    "critic_x_unsup, critic_x_sup = build_critic_x(\n",
    "    input_shape=critic_x_input_shape,\n",
    "    n_classes=n_classes\n",
    ")\n",
    "\n",
    "critic_z = build_critic_z(\n",
    "    input_shape=critic_z_input_shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "def _gradient_penalty_loss(y_true, y_pred, averaged_samples):\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr, axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "    return K.mean(gradient_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.trainable = False\n",
    "encoder.trainable = False\n",
    "\n",
    "x = Input(shape=input_shape)\n",
    "y = Input(shape=target_shape)\n",
    "z = Input(shape=(latent_dim, 1))\n",
    "\n",
    "x_ = generator(z)\n",
    "z_ = encoder(x)\n",
    "fake_x = critic_x_unsup(x_) # Fake\n",
    "valid_x = critic_x_unsup(y) # Truth\n",
    "\n",
    "label = critic_x_sup(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic X Unsupervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_33\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_45 (InputLayer)           [(None, 20, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_44 (InputLayer)           [(None, 100, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_28 (Model)                (None, 100, 1)       133787      input_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_6 (TensorFlowOp [(64, 100, 1)]       0           input_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_7 (TensorFlowOp [(64, 100, 1)]       0           model_28[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_3 (TensorFlowOp [(64, 100, 1)]       0           tf_op_layer_mul_6[0][0]          \n",
      "                                                                 tf_op_layer_mul_7[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "model_29 (Model)                multiple             67393       model_28[1][0]                   \n",
      "                                                                 input_44[0][0]                   \n",
      "                                                                 tf_op_layer_add_3[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 201,180\n",
      "Trainable params: 67,393\n",
      "Non-trainable params: 133,787\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "alpha = K.random_uniform((64, 1, 1))\n",
    "interpolated_x = (alpha * [y, x_][0]) + ((1 - alpha) * [y, x_][1])\n",
    "validity_interpolated_x = critic_x_unsup(interpolated_x)\n",
    "partial_gp_loss_x = partial(_gradient_penalty_loss, averaged_samples=interpolated_x)\n",
    "partial_gp_loss_x.__name__ = 'gradient_penalty'\n",
    "critic_x_unsup_model = Model(inputs=[y, z], outputs=[valid_x, fake_x,validity_interpolated_x])\n",
    "critic_x_unsup_model.compile(loss=[_wasserstein_loss, _wasserstein_loss, partial_gp_loss_x], \n",
    "                       optimizer=optimizer, loss_weights=[1, 1, 10])\n",
    "critic_x_unsup_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic X Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_41 (InputLayer)        [(None, 100, 1)]          0         \n",
      "_________________________________________________________________\n",
      "sequential_28 (Sequential)   multiple                  62016     \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 3)                 16131     \n",
      "=================================================================\n",
      "Total params: 78,147\n",
      "Trainable params: 78,147\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic_x_sup.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=optimizer, \n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "critic_x_sup_model = critic_x_sup\n",
    "critic_x_sup_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic Z Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_35\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_43 (InputLayer)           [(None, 100, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_45 (InputLayer)           [(None, 20, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_27 (Model)                (None, 20, 1)        481620      input_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_10 (TensorFlowO [(64, 20, 1)]        0           input_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_11 (TensorFlowO [(64, 20, 1)]        0           model_27[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_5 (TensorFlowOp [(64, 20, 1)]        0           tf_op_layer_mul_10[0][0]         \n",
      "                                                                 tf_op_layer_mul_11[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "model_31 (Model)                multiple             861         model_27[1][0]                   \n",
      "                                                                 input_45[0][0]                   \n",
      "                                                                 tf_op_layer_add_5[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 482,481\n",
      "Trainable params: 861\n",
      "Non-trainable params: 481,620\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fake_z = critic_z(z_)\n",
    "valid_z = critic_z(z)\n",
    "alpha = K.random_uniform((64, 1, 1))\n",
    "interpolated_z = (alpha * [z, z_][0]) + ((1 - alpha) * [z, z_][1])\n",
    "validity_interpolated_z = critic_z(interpolated_z)\n",
    "partial_gp_loss_z = partial(_gradient_penalty_loss, averaged_samples=interpolated_z)\n",
    "partial_gp_loss_z.__name__ = 'gradient_penalty'\n",
    "critic_z_model = tf.keras.Model(inputs=[x, z], outputs=[valid_z, fake_z,validity_interpolated_z])\n",
    "critic_z_model.compile(loss=[_wasserstein_loss, _wasserstein_loss,\n",
    "                                  partial_gp_loss_z], optimizer=optimizer,\n",
    "                            loss_weights=[1, 1, 10])\n",
    "critic_z_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_37\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_48 (InputLayer)           [(None, 20, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_49 (InputLayer)           [(None, 100, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_28 (Model)                (None, 100, 1)       133787      input_48[0][0]                   \n",
      "                                                                 model_27[3][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_27 (Model)                (None, 20, 1)        481620      input_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_29 (Model)                multiple             67393       model_28[4][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_31 (Model)                multiple             861         model_27[3][0]                   \n",
      "==================================================================================================\n",
      "Total params: 683,661\n",
      "Trainable params: 615,407\n",
      "Non-trainable params: 68,254\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic_x_sup.trainable = False\n",
    "critic_x_unsup.trainable = False\n",
    "critic_z.trainable = False\n",
    "generator.trainable = True\n",
    "encoder.trainable = True\n",
    "\n",
    "z_gen = Input(shape=(latent_dim, 1))\n",
    "x_gen_ = generator(z_gen)\n",
    "x_gen = Input(shape=input_shape)\n",
    "z_gen_ = encoder(x_gen)\n",
    "x_gen_rec = generator(z_gen_)\n",
    "fake_gen_x = critic_x_unsup(x_gen_)\n",
    "fake_gen_z = critic_z(z_gen_)\n",
    "\n",
    "encoder_generator_model = Model([x_gen, z_gen], [fake_gen_x, fake_gen_z, x_gen_rec])\n",
    "encoder_generator_model.compile(loss=[_wasserstein_loss, _wasserstein_loss,'mse'], \n",
    "                                optimizer=optimizer,\n",
    "                                loss_weights=[1, 1, 10])\n",
    "\n",
    "encoder_generator_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_supervised_samples(n_samples, X, labels, n_classes):\n",
    "    \"\"\"Select a supervised subset of the dataset, ensures classes are balanced.\"\"\"\n",
    "    X_list, y_list = list(), list()\n",
    "    n_per_class = int(n_samples / n_classes)\n",
    "    for i in range(n_classes):\n",
    "        X_with_class = X[labels == i]\n",
    "        ix = randint(0, len(X_with_class), n_per_class)\n",
    "        [X_list.append(X_with_class[j]) for j in ix]\n",
    "        [y_list.append(i) for j in ix]\n",
    "    return asarray(X_list), asarray(y_list)\n",
    "\n",
    "def generate_real_samples(n_samples, X, labels):\n",
    "    \"\"\"Randomly select real samples.\"\"\"\n",
    "    ix = randint(0, X.shape[0], n_samples)\n",
    "    X, labels = X[ix], labels[ix]\n",
    "    y = ones((n_samples, 1))\n",
    "    return [X, labels], y\n",
    "\n",
    "def generate_latent_points(n_samples, latent_dim):\n",
    "    \"\"\"Generate points in latent space as input for the generator.\"\"\"\n",
    "    return np.random.normal(size=(n_samples, latent_dim, 1))\n",
    " \n",
    "def generate_fake_samples(n_samples, latent_dim, generator):\n",
    "    \"\"\"Generate n fake examples with class labels.\"\"\"\n",
    "    z = generate_latent_points(n_samples, latent_dim)\n",
    "    x_ = generator(z)\n",
    "    y = zeros((n_samples, 1))\n",
    "    return x_, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = np.ones((batch_size, 1))\n",
    "valid = -np.ones((batch_size, 1))\n",
    "delta = np.ones((batch_size, 1))\n",
    "\n",
    "indices = np.arange(X_train.shape[0])\n",
    "for epoch in range(1, epochs + 1):\n",
    "    np.random.shuffle(indices)\n",
    "    X_ = X_train[indices]\n",
    "    y_ = y_train[indices]\n",
    "\n",
    "    epoch_g_loss = []\n",
    "    epoch_cx_loss = []\n",
    "    epoch_cz_loss = []\n",
    "\n",
    "    minibatches_size = batch_size * iterations_critic\n",
    "    num_minibatches = int(X_.shape[0] // minibatches_size)\n",
    "\n",
    "    for i in range(num_minibatches):\n",
    "        minibatch = X_[i * minibatches_size: (i + 1) * minibatches_size]\n",
    "        y_minibatch = y_[i * minibatches_size: (i + 1) * minibatches_size]\n",
    "\n",
    "        for j in range(iterations_critic):\n",
    "            x = minibatch[j * batch_size: (j + 1) * batch_size]\n",
    "            y = y_minibatch[j * batch_size: (j + 1) * batch_size]\n",
    "            z = np.random.normal(size=(batch_size, latent_dim, 1))\n",
    "            epoch_cx_loss.append(\n",
    "                critic_x_model.train_on_batch([y, z], [valid, fake, delta]))\n",
    "            epoch_cz_loss.append(\n",
    "                critic_z_model.train_on_batch([x, z], [valid, fake, delta]))\n",
    "\n",
    "        epoch_g_loss.append(\n",
    "            encoder_generator_model.train_on_batch([x, z], [valid, valid, y]))\n",
    "\n",
    "    cx_loss = np.mean(np.array(epoch_cx_loss), axis=0)\n",
    "    cz_loss = np.mean(np.array(epoch_cz_loss), axis=0)\n",
    "    g_loss = np.mean(np.array(epoch_g_loss), axis=0)\n",
    "    print('Epoch: {}/{}, [Dx loss: {}] [Dz loss: {}] [G loss: {}]'.format(\n",
    "        epoch, epochs, cx_loss, cz_loss, g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, critic_x_unsup, critic_x_sup, gan, X, labels, latent_dim, epochs, batch_size):\n",
    "    \n",
    "    X_sup, labels_sup = select_supervised_samples(batch_size, X, labels, n_classes)\n",
    "    num_batches = X.shape[0] // batch_size\n",
    "    half_batch = batch_size // 2\n",
    "    \n",
    "    for e in range(epochs):\n",
    "       \n",
    "        critic_x_losses = []\n",
    "        generator_losses = []\n",
    "        \n",
    "        for n in range(num_batches):\n",
    "            # Train critic x supervised model\n",
    "            [X_sup_real, y_sup_real], _ = generate_real_samples(half_batch, X_sup, labels_sup)\n",
    "            critix_x_sup_loss = list(critic_x_sup.train_on_batch(X_sup_real, y_sup_real))\n",
    "\n",
    "            # Train critic x unsupervised model with real examples\n",
    "            [X_unsup_real, _], ones_batch = generate_real_samples(half_batch, X, labels)\n",
    "            critix_x_unsup_real_loss = critic_x_unsup.train_on_batch(X_unsup_real, ones_batch)\n",
    "\n",
    "            # Train critic x unsupervised model with fake examples\n",
    "            X_unsup_fake, zeros_batch = generate_fake_samples(half_batch, latent_dim, generator)\n",
    "            critix_x_unsup_fake_loss = critic_x_unsup.train_on_batch(X_unsup_fake, zeros_batch)\n",
    "\n",
    "            critic_x_losses.append(critix_x_sup_loss + [critix_x_unsup_real_loss, critix_x_unsup_fake_loss])\n",
    "\n",
    "\n",
    "            # Train generator\n",
    "            X_gan, labels_gan = generate_latent_points(batch_size, latent_dim), ones((batch_size, 1))\n",
    "            generator_loss = gan.train_on_batch(X_gan, labels_gan)\n",
    "            \n",
    "            generator_losses.append([generator_loss])\n",
    "        \n",
    "        \n",
    "        critic_x_loss = np.array(critic_x_losses).mean(axis=0)\n",
    "        generator_loss = np.array(generator_losses).mean(axis=0)\n",
    "        print(f'epoch: {e}, critic_x: {critic_x_loss}, generator: {generator_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, critic_x: [0.7081537  0.61621094 0.57729495 0.7271075 ], generator: [1.2430649]\n",
      "epoch: 1, critic_x: [0.62288827 0.65527344 0.6921256  0.46636146], generator: [1.709136]\n",
      "epoch: 2, critic_x: [0.50837153 0.7236328  0.43268517 0.27707696], generator: [3.4420176]\n",
      "epoch: 3, critic_x: [0.3701978  0.7988281  0.09752154 0.07441215], generator: [4.580372]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_5/f7yzqnxs6694l3c_yk9mjpzr0000gn/T/ipykernel_5057/199339328.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_x_unsup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_x_sup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/_5/f7yzqnxs6694l3c_yk9mjpzr0000gn/T/ipykernel_5057/3935346792.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(generator, critic_x_unsup, critic_x_sup, gan, X, labels, latent_dim, epochs, batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# Train critic x unsupervised model with fake examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mX_unsup_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_fake_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mcritix_x_unsup_fake_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic_x_unsup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_unsup_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mcritic_x_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritix_x_sup_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcritix_x_unsup_real_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritix_x_unsup_fake_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hitlads-env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1015\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hitlads-env/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3471\u001b[0m         \u001b[0mfeed_symbols\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_symbols\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3472\u001b[0m         session != self._session):\n\u001b[0;32m-> 3473\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hitlads-env/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   3408\u001b[0m       \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3409\u001b[0m     \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3410\u001b[0;31m     \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3411\u001b[0m     \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3412\u001b[0m     \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hitlads-env/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1502\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m     \"\"\"\n\u001b[0;32m-> 1504\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1505\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hitlads-env/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    generator, \n",
    "    critic_x_unsup, \n",
    "    critic_x_sup, \n",
    "    gan, \n",
    "    X_train, \n",
    "    labels_train, \n",
    "    latent_dim, \n",
    "    epochs, \n",
    "    batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of unsupervised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_ = encoder.predict(X_train)\n",
    "y_hat = generator.predict(z_)\n",
    "critic = critic_x_unsup.predict(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Query Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What did the unsupervised GAN say was an anomaly?\n",
    "z_ = encoder.predict(X_train)\n",
    "y_hat = generator.predict(z_)\n",
    "critic = critic_x_unsup.predict(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What did the supervised GAN get wrong?\n",
    "\n",
    "# False Negatives\n",
    "label_hat = critic_x_unsup.predict(X_train[labels==1])\n",
    "\n",
    "# False Positives\n",
    "label_hat = critic_x_unsup.predict(X_train[labels==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How should we explore datasets?\n",
    "\n",
    "test = np.randint(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Annotation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we combine labels from different people?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
